Namespace(encoder='TurboAE_rate3_cnn', decoder='TurboAE_rate3_cnn', is_k_same_code=False, vv=5, radar_prob=0.05, radar_power=5.0, train_enc_channel_low=0.0, train_enc_channel_high=100, train_dec_channel_low=0.0, train_dec_channel_high=100.0, init_nw_weight='default', joint_train=0, enc_rnn='gru', dec_rnn='gru', enc_num_layer=3, dec_num_layer=3, dec_num_unit=100, enc_num_unit=25, enc_act='elu', dec_act='linear', num_train_dec=5, num_train_enc=1, num_train_mod=1, num_train_demod=5, num_iteration=6, enc_kernel_size=5, dec_kernel_size=5, extrinsic=1, num_iter_ft=5, mod_pc='qpsk', is_interleave=1, is_same_interleaver=1, mod_num_layer=1, mod_num_unit=20, demod_num_layer=1, demod_num_unit=20, mod_lr=0.005, demod_lr=0.005, dropout=0.0, snr_test_start=0, snr_test_end=100, snr_points=20, batch_size=100, num_epoch=10, test_ratio=1, block_len=(10, 20), code_rate_k=1, code_rate_n=3, channel=('awgn', 'fading'), modtype=('QAM2', 'QAM4'), mod_rate=(2, 4), block_len_low=10, block_len_high=200, is_variable_block_len=False, num_block=5000, test_channel_mode='block_norm', train_channel_mode='block_norm', enc_truncate_limit=0, no_code_norm=False, enc_quantize_level=2, enc_value_limit=1.0, enc_grad_limit=0.01, enc_clipping='both', optimizer='adam', dec_lr=0.01, enc_lr=0.01, no_cuda=False, rec_quantize=False, print_pos_ber=False, print_pos_power=False, print_test_traj=False, precompute_norm_stats=False, D=1, BASE_PATH='C:\\WorkSpace\\FadingChannels\\Swetha_M20AIE317_MTP\\Fading', LOG_PATH='C:\\WorkSpace\\FadingChannels\\Swetha_M20AIE317_MTP\\Fading\\20230726_035537\\logs_faded', DATA_PATH='C:\\WorkSpace\\FadingChannels\\Swetha_M20AIE317_MTP\\Fading\\20230726_035537\\data_faded', MODEL_PATH='C:\\WorkSpace\\FadingChannels\\Swetha_M20AIE317_MTP\\Fading\\20230726_035537\\model_faded', PLOT_PATH='C:\\WorkSpace\\FadingChannels\\Swetha_M20AIE317_MTP\\Fading\\20230726_035537\\plot_faded')
use_cuda:  False

 
 ###############################################################################################
Training Model for : block length => 10 coderate_k => 1 coderate_n => 3 channel => awgn mod type=> QAM2
using random interleaver [2 8 4 9 1 6 7 3 0 5] [3 5 1 2 9 8 0 6 7 4]
Channel_ModAE(
  (enc): ENC_interCNN(
    (enc_cnn_1): SameShapeConv1d(
      (cnns): ModuleList(
        (0): Conv1d(1, 25, kernel_size=(5,), stride=(1,), padding=(2,))
        (1): Conv1d(25, 25, kernel_size=(5,), stride=(1,), padding=(2,))
        (2): Conv1d(25, 25, kernel_size=(5,), stride=(1,), padding=(2,))
      )
    )
    (enc_cnn_2): SameShapeConv1d(
      (cnns): ModuleList(
        (0): Conv1d(1, 25, kernel_size=(5,), stride=(1,), padding=(2,))
        (1): Conv1d(25, 25, kernel_size=(5,), stride=(1,), padding=(2,))
        (2): Conv1d(25, 25, kernel_size=(5,), stride=(1,), padding=(2,))
      )
    )
    (enc_cnn_3): SameShapeConv1d(
      (cnns): ModuleList(
        (0): Conv1d(1, 25, kernel_size=(5,), stride=(1,), padding=(2,))
        (1): Conv1d(25, 25, kernel_size=(5,), stride=(1,), padding=(2,))
        (2): Conv1d(25, 25, kernel_size=(5,), stride=(1,), padding=(2,))
      )
    )
    (enc_linear_1): Linear(in_features=25, out_features=1, bias=True)
    (enc_linear_2): Linear(in_features=25, out_features=1, bias=True)
    (enc_linear_3): Linear(in_features=25, out_features=1, bias=True)
    (interleaver): Interleaver()
  )
  (dec): DEC_LargeCNN(
    (interleaver): Interleaver()
    (deinterleaver): DeInterleaver()
    (dec1_cnns): ModuleList(
      (0): SameShapeConv1d(
        (cnns): ModuleList(
          (0): Conv1d(7, 100, kernel_size=(5,), stride=(1,), padding=(2,))
          (1): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,))
          (2): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,))
        )
      )
      (1): SameShapeConv1d(
        (cnns): ModuleList(
          (0): Conv1d(7, 100, kernel_size=(5,), stride=(1,), padding=(2,))
          (1): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,))
          (2): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,))
        )
      )
      (2): SameShapeConv1d(
        (cnns): ModuleList(
          (0): Conv1d(7, 100, kernel_size=(5,), stride=(1,), padding=(2,))
          (1): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,))
          (2): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,))
        )
      )
      (3): SameShapeConv1d(
        (cnns): ModuleList(
          (0): Conv1d(7, 100, kernel_size=(5,), stride=(1,), padding=(2,))
          (1): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,))
          (2): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,))
        )
      )
      (4): SameShapeConv1d(
        (cnns): ModuleList(
          (0): Conv1d(7, 100, kernel_size=(5,), stride=(1,), padding=(2,))
          (1): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,))
          (2): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,))
        )
      )
      (5): SameShapeConv1d(
        (cnns): ModuleList(
          (0): Conv1d(7, 100, kernel_size=(5,), stride=(1,), padding=(2,))
          (1): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,))
          (2): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,))
        )
      )
    )
    (dec2_cnns): ModuleList(
      (0): SameShapeConv1d(
        (cnns): ModuleList(
          (0): Conv1d(7, 100, kernel_size=(5,), stride=(1,), padding=(2,))
          (1): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,))
          (2): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,))
        )
      )
      (1): SameShapeConv1d(
        (cnns): ModuleList(
          (0): Conv1d(7, 100, kernel_size=(5,), stride=(1,), padding=(2,))
          (1): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,))
          (2): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,))
        )
      )
      (2): SameShapeConv1d(
        (cnns): ModuleList(
          (0): Conv1d(7, 100, kernel_size=(5,), stride=(1,), padding=(2,))
          (1): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,))
          (2): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,))
        )
      )
      (3): SameShapeConv1d(
        (cnns): ModuleList(
          (0): Conv1d(7, 100, kernel_size=(5,), stride=(1,), padding=(2,))
          (1): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,))
          (2): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,))
        )
      )
      (4): SameShapeConv1d(
        (cnns): ModuleList(
          (0): Conv1d(7, 100, kernel_size=(5,), stride=(1,), padding=(2,))
          (1): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,))
          (2): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,))
        )
      )
      (5): SameShapeConv1d(
        (cnns): ModuleList(
          (0): Conv1d(7, 100, kernel_size=(5,), stride=(1,), padding=(2,))
          (1): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,))
          (2): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,))
        )
      )
    )
    (dec1_outputs): ModuleList(
      (0): Linear(in_features=100, out_features=5, bias=True)
      (1): Linear(in_features=100, out_features=5, bias=True)
      (2): Linear(in_features=100, out_features=5, bias=True)
      (3): Linear(in_features=100, out_features=5, bias=True)
      (4): Linear(in_features=100, out_features=5, bias=True)
      (5): Linear(in_features=100, out_features=5, bias=True)
    )
    (dec2_outputs): ModuleList(
      (0): Linear(in_features=100, out_features=5, bias=True)
      (1): Linear(in_features=100, out_features=5, bias=True)
      (2): Linear(in_features=100, out_features=5, bias=True)
      (3): Linear(in_features=100, out_features=5, bias=True)
      (4): Linear(in_features=100, out_features=5, bias=True)
      (5): Linear(in_features=100, out_features=1, bias=True)
    )
  )
  (mod): Modulation(
    (mod_layer): SameShapeConv1d(
      (cnns): ModuleList(
        (0): Conv1d(2, 20, kernel_size=(1,), stride=(1,))
      )
    )
    (mod_final): SameShapeConv1d(
      (cnns): ModuleList(
        (0): Conv1d(20, 2, kernel_size=(1,), stride=(1,))
      )
    )
  )
  (demod): DeModulation(
    (demod_layer): SameShapeConv1d(
      (cnns): ModuleList(
        (0): Conv1d(2, 20, kernel_size=(1,), stride=(1,))
      )
    )
    (demod_final): SameShapeConv1d(
      (cnns): ModuleList(
        (0): Conv1d(20, 2, kernel_size=(1,), stride=(1,))
      )
    )
  )
)
====> Epoch: 1 Average loss: 0.68725344  running time 18.216602325439453
====> Epoch: 1 Average loss: 46.00432221  running time 19.969687700271606
====> Epoch: 1 Average loss: 50.01200005  running time 21.429754495620728
====> Epoch: 1 Average loss: 50.12000015  running time 22.23238730430603
====> Epoch: 1 Average loss: 49.71799980  running time 21.936753749847412
====> Epoch: 1 Average loss: 50.04800003  running time 26.787840604782104
====> Epoch: 1 Average loss: 50.20200020  running time 23.26940894126892
====> Epoch: 1 Average loss: 49.92799988  running time 20.49548363685608
====> Epoch: 1 Average loss: 49.90399994  running time 20.692210912704468
====> Epoch: 1 Average loss: 50.32799980  running time 20.499107837677002
====> Epoch: 1 Average loss: 50.49799995  running time 20.364965438842773
====> Epoch: 1 Average loss: 49.94200012  running time 20.45474624633789
====> Test set BCE loss 49.88399887084961 Custom Loss 49.88399887084961 with ber  0.4988398849964142
====> Epoch: 2 Average loss: 49.80600006  running time 20.41438913345337
====> Epoch: 2 Average loss: 49.94599983  running time 21.07739281654358
====> Epoch: 2 Average loss: 49.93200005  running time 21.243333101272583
====> Epoch: 2 Average loss: 50.08599991  running time 21.19278049468994
====> Epoch: 2 Average loss: 49.76599998  running time 22.15081286430359
====> Epoch: 2 Average loss: 50.32199989  running time 21.39914560317993
====> Epoch: 2 Average loss: 50.12400032  running time 20.603236198425293
====> Epoch: 2 Average loss: 49.93800018  running time 20.491071462631226
====> Epoch: 2 Average loss: 49.99400009  running time 20.756956815719604
====> Epoch: 2 Average loss: 50.27799995  running time 20.539886236190796
====> Epoch: 2 Average loss: 49.75399994  running time 20.634838581085205
====> Epoch: 2 Average loss: 50.28599968  running time 20.39082098007202
====> Test set BCE loss 50.31599426269531 Custom Loss 50.31599426269531 with ber  0.5031600594520569
====> Epoch: 3 Average loss: 50.01799980  running time 20.377259731292725
====> Epoch: 3 Average loss: 50.20400017  running time 21.26624321937561
====> Epoch: 3 Average loss: 50.06599991  running time 21.196909427642822
====> Epoch: 3 Average loss: 49.79400024  running time 21.723103523254395
====> Epoch: 3 Average loss: 50.21199982  running time 21.573644876480103
====> Epoch: 3 Average loss: 50.20400002  running time 21.86673665046692
====> Epoch: 3 Average loss: 50.19999992  running time 20.63267493247986
====> Epoch: 3 Average loss: 49.98400024  running time 20.625866413116455
====> Epoch: 3 Average loss: 50.11199997  running time 20.305099487304688
====> Epoch: 3 Average loss: 50.02000008  running time 20.433998346328735
====> Epoch: 3 Average loss: 50.74200012  running time 20.45841670036316
====> Epoch: 3 Average loss: 50.09400002  running time 20.34603214263916
====> Test set BCE loss 49.564002990722656 Custom Loss 49.564002990722656 with ber  0.49563995003700256
====> Epoch: 4 Average loss: 49.76000015  running time 20.476318836212158
====> Epoch: 4 Average loss: 49.91199989  running time 22.926909923553467
====> Epoch: 4 Average loss: 49.81400009  running time 23.748425245285034
====> Epoch: 4 Average loss: 50.18200027  running time 23.574652433395386
====> Epoch: 4 Average loss: 50.28600021  running time 23.235764980316162
====> Epoch: 4 Average loss: 50.13400002  running time 23.408287525177002
====> Epoch: 4 Average loss: 49.89199989  running time 20.76179599761963
====> Epoch: 4 Average loss: 49.62400002  running time 20.765005111694336
====> Epoch: 4 Average loss: 49.71199951  running time 21.624271869659424
====> Epoch: 4 Average loss: 50.11600014  running time 20.49829125404358
====> Epoch: 4 Average loss: 49.87400017  running time 20.377004623413086
====> Epoch: 4 Average loss: 49.92199989  running time 20.397422552108765
====> Test set BCE loss 50.04999923706055 Custom Loss 50.04999923706055 with ber  0.500499963760376
====> Epoch: 5 Average loss: 50.16200005  running time 20.40800404548645
====> Epoch: 5 Average loss: 50.17000000  running time 22.790003776550293
====> Epoch: 5 Average loss: 49.91799995  running time 22.85589027404785
====> Epoch: 5 Average loss: 50.00400002  running time 29.565016508102417
====> Epoch: 5 Average loss: 49.76800003  running time 23.06139326095581
====> Epoch: 5 Average loss: 49.65800026  running time 23.311668395996094
====> Epoch: 5 Average loss: 50.09199997  running time 21.119844675064087
====> Epoch: 5 Average loss: 49.81600006  running time 20.842870712280273
====> Epoch: 5 Average loss: 49.95800011  running time 20.794447660446167
====> Epoch: 5 Average loss: 50.42799988  running time 20.707226753234863
====> Epoch: 5 Average loss: 50.34199997  running time 20.25652837753296
====> Epoch: 5 Average loss: 50.22399994  running time 20.20061492919922
====> Test set BCE loss 49.721988677978516 Custom Loss 49.721988677978516 with ber  0.4972200393676758
====> Epoch: 6 Average loss: 50.43999977  running time 20.29539704322815
====> Epoch: 6 Average loss: 50.12399986  running time 23.372910499572754
====> Epoch: 6 Average loss: 49.92599976  running time 23.22548818588257
====> Epoch: 6 Average loss: 49.69800011  running time 23.5321786403656
====> Epoch: 6 Average loss: 50.31400002  running time 23.589263200759888
====> Epoch: 6 Average loss: 50.45000000  running time 22.87873601913452
====> Epoch: 6 Average loss: 50.05399994  running time 20.246783018112183
====> Epoch: 6 Average loss: 49.74800018  running time 21.489896535873413
====> Epoch: 6 Average loss: 49.88200012  running time 20.536406993865967
====> Epoch: 6 Average loss: 49.90000015  running time 20.18156337738037
====> Epoch: 6 Average loss: 49.95399986  running time 20.35132122039795
====> Epoch: 6 Average loss: 50.46200012  running time 20.664508819580078
====> Test set BCE loss 50.018001556396484 Custom Loss 50.018001556396484 with ber  0.5001800060272217
====> Epoch: 7 Average loss: 50.11800011  running time 20.36126685142517
====> Epoch: 7 Average loss: 50.05999992  running time 22.69907832145691
====> Epoch: 7 Average loss: 50.07600014  running time 22.51084876060486
====> Epoch: 7 Average loss: 49.99000023  running time 22.649564504623413
====> Epoch: 7 Average loss: 50.27799995  running time 24.718786239624023
====> Epoch: 7 Average loss: 49.90199997  running time 22.498682260513306
====> Epoch: 7 Average loss: 49.65800003  running time 20.295589923858643
====> Epoch: 7 Average loss: 49.95200020  running time 20.298245429992676
====> Epoch: 7 Average loss: 49.77399994  running time 20.264853477478027
====> Epoch: 7 Average loss: 50.03799995  running time 20.315237283706665
====> Epoch: 7 Average loss: 49.81600014  running time 20.21457576751709
====> Epoch: 7 Average loss: 49.82799995  running time 20.40658688545227
====> Test set BCE loss 50.36600112915039 Custom Loss 50.36600112915039 with ber  0.5036600232124329
====> Epoch: 8 Average loss: 49.85400002  running time 20.605364084243774
====> Epoch: 8 Average loss: 49.91200005  running time 22.8300838470459
====> Epoch: 8 Average loss: 50.10800018  running time 22.5866277217865
====> Epoch: 8 Average loss: 50.16999985  running time 22.66577982902527
====> Epoch: 8 Average loss: 49.57799995  running time 22.622126579284668
====> Epoch: 8 Average loss: 49.87200005  running time 22.700207471847534
====> Epoch: 8 Average loss: 49.88000000  running time 20.337805032730103
====> Epoch: 8 Average loss: 50.19000008  running time 20.304625749588013
====> Epoch: 8 Average loss: 50.39000015  running time 20.202532052993774
====> Epoch: 8 Average loss: 50.56400002  running time 20.295756578445435
====> Epoch: 8 Average loss: 49.97599991  running time 20.295194149017334
====> Epoch: 8 Average loss: 49.77600014  running time 20.185689687728882
====> Test set BCE loss 50.14400863647461 Custom Loss 50.14400863647461 with ber  0.5014399886131287
====> Epoch: 9 Average loss: 49.79799995  running time 20.44188690185547
====> Epoch: 9 Average loss: 49.87800011  running time 22.46685266494751
====> Epoch: 9 Average loss: 50.17399994  running time 22.7570903301239
====> Epoch: 9 Average loss: 50.09599991  running time 22.67753791809082
====> Epoch: 9 Average loss: 49.78600014  running time 22.73759365081787
====> Epoch: 9 Average loss: 50.22200012  running time 22.597769737243652
====> Epoch: 9 Average loss: 50.07199989  running time 20.21021604537964
====> Epoch: 9 Average loss: 49.63599983  running time 20.196789503097534
====> Epoch: 9 Average loss: 50.00800011  running time 20.211777687072754
====> Epoch: 9 Average loss: 50.06800026  running time 20.242172718048096
====> Epoch: 9 Average loss: 50.06399979  running time 20.90075421333313
====> Epoch: 9 Average loss: 50.23600014  running time 20.669240713119507
====> Test set BCE loss 50.361995697021484 Custom Loss 50.361995697021484 with ber  0.5036200284957886
====> Epoch: 10 Average loss: 50.12600006  running time 20.92124915122986
====> Epoch: 10 Average loss: 49.91000008  running time 22.982171773910522
====> Epoch: 10 Average loss: 49.71999992  running time 23.205846786499023
====> Epoch: 10 Average loss: 49.93799995  running time 23.25809645652771
====> Epoch: 10 Average loss: 49.81399994  running time 23.042670011520386
====> Epoch: 10 Average loss: 50.04199997  running time 23.21676206588745
====> Epoch: 10 Average loss: 50.01800018  running time 20.677916765213013
====> Epoch: 10 Average loss: 50.24199997  running time 20.65845799446106
====> Epoch: 10 Average loss: 49.74599998  running time 20.09303307533264
====> Epoch: 10 Average loss: 49.60399986  running time 20.116854190826416
====> Epoch: 10 Average loss: 49.87599998  running time 20.189899682998657
====> Epoch: 10 Average loss: 49.83600014  running time 20.237828254699707
====> Test set BCE loss 49.742008209228516 Custom Loss 49.742008209228516 with ber  0.4974200427532196
saved model C:\WorkSpace\FadingChannels\Swetha_M20AIE317_MTP\Fading\20230726_035537\model_faded\bl_10__k_1_n_3_chl_awgn_mod_QAM2\attention_model_10_awgn_lr_0.01_D1bl_10__k_1_n_3_chl_awgn_mod_QAM2_5000_20230726_035537.pt
SNRS [0.0, 5.0, 10.0, 15.0, 20.0, 25.0, 30.0, 35.0, 40.0, 45.0, 50.0, 55.0, 60.0, 65.0, 70.0, 75.0, 80.0, 85.0, 90.0, 95.0]
no pos BER specified.
Test SNR 0.0 with ber  0.49729999899864197 with bler 0.9976
Punctured Test SNR 0.0 with ber  0.0 with bler 0.0
no pos BER specified.
Test SNR 5.0 with ber  0.4984798729419708 with bler 0.9994
Punctured Test SNR 5.0 with ber  0.0 with bler 0.0
no pos BER specified.
Test SNR 10.0 with ber  0.5007400512695312 with bler 0.9989999999999999
Punctured Test SNR 10.0 with ber  0.0 with bler 0.0
no pos BER specified.
Test SNR 15.0 with ber  0.4982600510120392 with bler 0.9994
Punctured Test SNR 15.0 with ber  0.0 with bler 0.0
no pos BER specified.
Test SNR 20.0 with ber  0.5055199265480042 with bler 1.0
Punctured Test SNR 20.0 with ber  0.0 with bler 0.0
no pos BER specified.
Test SNR 25.0 with ber  0.5026999711990356 with bler 0.9996
Punctured Test SNR 25.0 with ber  0.0 with bler 0.0
no pos BER specified.
Test SNR 30.0 with ber  0.5004198551177979 with bler 0.9992
Punctured Test SNR 30.0 with ber  0.0 with bler 0.0
no pos BER specified.
Test SNR 35.0 with ber  0.5017199516296387 with bler 0.9982000000000002
Punctured Test SNR 35.0 with ber  0.0 with bler 0.0
no pos BER specified.
Test SNR 40.0 with ber  0.4961400330066681 with bler 0.9996
Punctured Test SNR 40.0 with ber  0.0 with bler 0.0
no pos BER specified.
Test SNR 45.0 with ber  0.49807989597320557 with bler 0.9986000000000002
Punctured Test SNR 45.0 with ber  0.0 with bler 0.0
no pos BER specified.
Test SNR 50.0 with ber  0.49897998571395874 with bler 0.9994000000000001
Punctured Test SNR 50.0 with ber  0.0 with bler 0.0
no pos BER specified.
Test SNR 55.0 with ber  0.4985600411891937 with bler 0.9986000000000002
Punctured Test SNR 55.0 with ber  0.0 with bler 0.0
no pos BER specified.
Test SNR 60.0 with ber  0.49988001585006714 with bler 0.9986000000000002
Punctured Test SNR 60.0 with ber  0.0 with bler 0.0
no pos BER specified.
Test SNR 65.0 with ber  0.4975000321865082 with bler 0.9994
Punctured Test SNR 65.0 with ber  0.0 with bler 0.0
no pos BER specified.
Test SNR 70.0 with ber  0.4987199306488037 with bler 0.9982000000000001
Punctured Test SNR 70.0 with ber  0.0 with bler 0.0
no pos BER specified.
Test SNR 75.0 with ber  0.49775996804237366 with bler 0.9989999999999999
Punctured Test SNR 75.0 with ber  0.0 with bler 0.0
no pos BER specified.
Test SNR 80.0 with ber  0.49758008122444153 with bler 0.9986
Punctured Test SNR 80.0 with ber  0.0 with bler 0.0
no pos BER specified.
Test SNR 85.0 with ber  0.4946199655532837 with bler 0.9987999999999999
Punctured Test SNR 85.0 with ber  0.0 with bler 0.0
no pos BER specified.
Test SNR 90.0 with ber  0.5007200837135315 with bler 0.9992
Punctured Test SNR 90.0 with ber  0.0 with bler 0.0
no pos BER specified.
Test SNR 95.0 with ber  0.49886006116867065 with bler 0.9984000000000002
Punctured Test SNR 95.0 with ber  0.0 with bler 0.0
final results on SNRs  [0.0, 5.0, 10.0, 15.0, 20.0, 25.0, 30.0, 35.0, 40.0, 45.0, 50.0, 55.0, 60.0, 65.0, 70.0, 75.0, 80.0, 85.0, 90.0, 95.0]
BER [0.49729999899864197, 0.4984798729419708, 0.5007400512695312, 0.4982600510120392, 0.5055199265480042, 0.5026999711990356, 0.5004198551177979, 0.5017199516296387, 0.4961400330066681, 0.49807989597320557, 0.49897998571395874, 0.4985600411891937, 0.49988001585006714, 0.4975000321865082, 0.4987199306488037, 0.49775996804237366, 0.49758008122444153, 0.4946199655532837, 0.5007200837135315, 0.49886006116867065]
BLER [0.9976, 0.9994, 0.9989999999999999, 0.9994, 1.0, 0.9996, 0.9992, 0.9982000000000002, 0.9996, 0.9986000000000002, 0.9994000000000001, 0.9986000000000002, 0.9986000000000002, 0.9994, 0.9982000000000001, 0.9989999999999999, 0.9986, 0.9987999999999999, 0.9992, 0.9984000000000002]
final results on punctured SNRs  [0.0, 5.0, 10.0, 15.0, 20.0, 25.0, 30.0, 35.0, 40.0, 45.0, 50.0, 55.0, 60.0, 65.0, 70.0, 75.0, 80.0, 85.0, 90.0, 95.0]
BER [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
BLER [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
encoder power is tensor(1.)
adjusted SNR should be [-0.0, 4.999999888090176, 10.00000005838476, 15.000000078774018, 19.999999870570157, 25.000000003171387, 30.000000467677864, 34.99999989681464, 40.00000019414476, 45.00000000317138, 50.000000083965574, 54.99999989681464, 59.99999958744239, 65.00000036280017, 70.00000024384569, 75.0000000389704, 80.00000021942404, 84.9999998008802, 89.99999964429526, 95.0000003943598]

 
 ###############################################################################################
Training Model for : block length => 10 coderate_k => 1 coderate_n => 3 channel => fading mod type=> QAM2
using random interleaver [2 8 4 9 1 6 7 3 0 5] [3 5 1 2 9 8 0 6 7 4]
Channel_ModAE(
  (enc): ENC_interCNN(
    (enc_cnn_1): SameShapeConv1d(
      (cnns): ModuleList(
        (0): Conv1d(1, 25, kernel_size=(5,), stride=(1,), padding=(2,))
        (1): Conv1d(25, 25, kernel_size=(5,), stride=(1,), padding=(2,))
        (2): Conv1d(25, 25, kernel_size=(5,), stride=(1,), padding=(2,))
      )
    )
    (enc_cnn_2): SameShapeConv1d(
      (cnns): ModuleList(
        (0): Conv1d(1, 25, kernel_size=(5,), stride=(1,), padding=(2,))
        (1): Conv1d(25, 25, kernel_size=(5,), stride=(1,), padding=(2,))
        (2): Conv1d(25, 25, kernel_size=(5,), stride=(1,), padding=(2,))
      )
    )
    (enc_cnn_3): SameShapeConv1d(
      (cnns): ModuleList(
        (0): Conv1d(1, 25, kernel_size=(5,), stride=(1,), padding=(2,))
        (1): Conv1d(25, 25, kernel_size=(5,), stride=(1,), padding=(2,))
        (2): Conv1d(25, 25, kernel_size=(5,), stride=(1,), padding=(2,))
      )
    )
    (enc_linear_1): Linear(in_features=25, out_features=1, bias=True)
    (enc_linear_2): Linear(in_features=25, out_features=1, bias=True)
    (enc_linear_3): Linear(in_features=25, out_features=1, bias=True)
    (interleaver): Interleaver()
  )
  (dec): DEC_LargeCNN(
    (interleaver): Interleaver()
    (deinterleaver): DeInterleaver()
    (dec1_cnns): ModuleList(
      (0): SameShapeConv1d(
        (cnns): ModuleList(
          (0): Conv1d(7, 100, kernel_size=(5,), stride=(1,), padding=(2,))
          (1): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,))
          (2): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,))
        )
      )
      (1): SameShapeConv1d(
        (cnns): ModuleList(
          (0): Conv1d(7, 100, kernel_size=(5,), stride=(1,), padding=(2,))
          (1): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,))
          (2): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,))
        )
      )
      (2): SameShapeConv1d(
        (cnns): ModuleList(
          (0): Conv1d(7, 100, kernel_size=(5,), stride=(1,), padding=(2,))
          (1): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,))
          (2): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,))
        )
      )
      (3): SameShapeConv1d(
        (cnns): ModuleList(
          (0): Conv1d(7, 100, kernel_size=(5,), stride=(1,), padding=(2,))
          (1): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,))
          (2): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,))
        )
      )
      (4): SameShapeConv1d(
        (cnns): ModuleList(
          (0): Conv1d(7, 100, kernel_size=(5,), stride=(1,), padding=(2,))
          (1): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,))
          (2): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,))
        )
      )
      (5): SameShapeConv1d(
        (cnns): ModuleList(
          (0): Conv1d(7, 100, kernel_size=(5,), stride=(1,), padding=(2,))
          (1): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,))
          (2): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,))
        )
      )
    )
    (dec2_cnns): ModuleList(
      (0): SameShapeConv1d(
        (cnns): ModuleList(
          (0): Conv1d(7, 100, kernel_size=(5,), stride=(1,), padding=(2,))
          (1): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,))
          (2): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,))
        )
      )
      (1): SameShapeConv1d(
        (cnns): ModuleList(
          (0): Conv1d(7, 100, kernel_size=(5,), stride=(1,), padding=(2,))
          (1): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,))
          (2): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,))
        )
      )
      (2): SameShapeConv1d(
        (cnns): ModuleList(
          (0): Conv1d(7, 100, kernel_size=(5,), stride=(1,), padding=(2,))
          (1): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,))
          (2): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,))
        )
      )
      (3): SameShapeConv1d(
        (cnns): ModuleList(
          (0): Conv1d(7, 100, kernel_size=(5,), stride=(1,), padding=(2,))
          (1): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,))
          (2): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,))
        )
      )
      (4): SameShapeConv1d(
        (cnns): ModuleList(
          (0): Conv1d(7, 100, kernel_size=(5,), stride=(1,), padding=(2,))
          (1): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,))
          (2): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,))
        )
      )
      (5): SameShapeConv1d(
        (cnns): ModuleList(
          (0): Conv1d(7, 100, kernel_size=(5,), stride=(1,), padding=(2,))
          (1): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,))
          (2): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,))
        )
      )
    )
    (dec1_outputs): ModuleList(
      (0): Linear(in_features=100, out_features=5, bias=True)
      (1): Linear(in_features=100, out_features=5, bias=True)
      (2): Linear(in_features=100, out_features=5, bias=True)
      (3): Linear(in_features=100, out_features=5, bias=True)
      (4): Linear(in_features=100, out_features=5, bias=True)
      (5): Linear(in_features=100, out_features=5, bias=True)
    )
    (dec2_outputs): ModuleList(
      (0): Linear(in_features=100, out_features=5, bias=True)
      (1): Linear(in_features=100, out_features=5, bias=True)
      (2): Linear(in_features=100, out_features=5, bias=True)
      (3): Linear(in_features=100, out_features=5, bias=True)
      (4): Linear(in_features=100, out_features=5, bias=True)
      (5): Linear(in_features=100, out_features=1, bias=True)
    )
  )
  (mod): Modulation(
    (mod_layer): SameShapeConv1d(
      (cnns): ModuleList(
        (0): Conv1d(2, 20, kernel_size=(1,), stride=(1,))
      )
    )
    (mod_final): SameShapeConv1d(
      (cnns): ModuleList(
        (0): Conv1d(20, 2, kernel_size=(1,), stride=(1,))
      )
    )
  )
  (demod): DeModulation(
    (demod_layer): SameShapeConv1d(
      (cnns): ModuleList(
        (0): Conv1d(2, 20, kernel_size=(1,), stride=(1,))
      )
    )
    (demod_final): SameShapeConv1d(
      (cnns): ModuleList(
        (0): Conv1d(20, 2, kernel_size=(1,), stride=(1,))
      )
    )
  )
)
====> Epoch: 1 Average loss: 0.69096170  running time 18.55528950691223
====> Epoch: 1 Average loss: 46.90271603  running time 29.908891916275024
====> Epoch: 1 Average loss: 49.91000000  running time 30.274718761444092
====> Epoch: 1 Average loss: 50.07000000  running time 30.044685125350952
====> Epoch: 1 Average loss: 50.39000008  running time 30.871710300445557
====> Epoch: 1 Average loss: 50.02399994  running time 33.319894313812256
====> Epoch: 1 Average loss: 50.40800003  running time 37.62162899971008
====> Epoch: 1 Average loss: 49.77199982  running time 33.34642004966736
====> Epoch: 1 Average loss: 50.24400009  running time 21.91060519218445
====> Epoch: 1 Average loss: 49.75799980  running time 20.211416006088257
====> Epoch: 1 Average loss: 50.35599976  running time 20.093569040298462
====> Epoch: 1 Average loss: 49.79600037  running time 20.147215127944946
====> Test set BCE loss 49.41999435424805 Custom Loss 49.41999435424805 with ber  0.4941999912261963
====> Epoch: 2 Average loss: 50.31400002  running time 20.3436541557312
====> Epoch: 2 Average loss: 50.01000008  running time 20.99646234512329
====> Epoch: 2 Average loss: 49.95800003  running time 20.979079961776733
====> Epoch: 2 Average loss: 49.82999985  running time 21.027358770370483
====> Epoch: 2 Average loss: 49.86600029  running time 20.873512744903564
====> Epoch: 2 Average loss: 49.76600014  running time 20.93607497215271
====> Epoch: 2 Average loss: 50.19799995  running time 21.426733016967773
====> Epoch: 2 Average loss: 50.01799995  running time 20.164069414138794
====> Epoch: 2 Average loss: 50.00000015  running time 20.306046962738037
====> Epoch: 2 Average loss: 49.84399971  running time 20.345858097076416
====> Epoch: 2 Average loss: 49.38000023  running time 20.356276035308838
====> Epoch: 2 Average loss: 50.16400009  running time 20.09343123435974
====> Test set BCE loss 50.197998046875 Custom Loss 50.197998046875 with ber  0.5019799470901489
====> Epoch: 3 Average loss: 49.83199974  running time 20.360506772994995
====> Epoch: 3 Average loss: 50.06800003  running time 21.004202604293823
====> Epoch: 3 Average loss: 50.01800018  running time 20.850258111953735
====> Epoch: 3 Average loss: 50.24400002  running time 21.178584814071655
====> Epoch: 3 Average loss: 49.92600006  running time 21.48485279083252
====> Epoch: 3 Average loss: 49.70400002  running time 21.66757607460022
====> Epoch: 3 Average loss: 49.96199982  running time 20.10668134689331
====> Epoch: 3 Average loss: 50.49199997  running time 20.11018419265747
====> Epoch: 3 Average loss: 49.94399994  running time 20.056117296218872
====> Epoch: 3 Average loss: 49.99200012  running time 20.040995836257935
====> Epoch: 3 Average loss: 50.05399994  running time 20.198686361312866
====> Epoch: 3 Average loss: 49.88200005  running time 20.027822256088257
====> Test set BCE loss 49.70000457763672 Custom Loss 49.70000457763672 with ber  0.4970000982284546
====> Epoch: 4 Average loss: 50.34599998  running time 20.217750787734985
====> Epoch: 4 Average loss: 49.61000008  running time 22.69852590560913
====> Epoch: 4 Average loss: 50.01999969  running time 22.885894775390625
====> Epoch: 4 Average loss: 49.86599976  running time 23.502175331115723
====> Epoch: 4 Average loss: 50.30800011  running time 22.608110666275024
====> Epoch: 4 Average loss: 49.86000015  running time 22.501776695251465
====> Epoch: 4 Average loss: 49.77800026  running time 19.978392124176025
====> Epoch: 4 Average loss: 50.13800011  running time 20.06717801094055
====> Epoch: 4 Average loss: 50.03400017  running time 20.319152355194092
====> Epoch: 4 Average loss: 49.87000008  running time 20.178946495056152
====> Epoch: 4 Average loss: 50.03399971  running time 20.095448970794678
====> Epoch: 4 Average loss: 50.15600006  running time 20.024210453033447
====> Test set BCE loss 50.06999206542969 Custom Loss 50.06999206542969 with ber  0.5006999969482422
====> Epoch: 5 Average loss: 49.87599991  running time 20.1160945892334
====> Epoch: 5 Average loss: 49.90400002  running time 22.520467281341553
====> Epoch: 5 Average loss: 49.85800011  running time 22.5277042388916
====> Epoch: 5 Average loss: 50.25600021  running time 22.541460514068604
====> Epoch: 5 Average loss: 50.08600029  running time 22.492077350616455
====> Epoch: 5 Average loss: 50.26799988  running time 22.58130717277527
====> Epoch: 5 Average loss: 49.97800011  running time 20.104560136795044
====> Epoch: 5 Average loss: 50.07000015  running time 20.13736367225647
====> Epoch: 5 Average loss: 49.67000008  running time 20.55399441719055
====> Epoch: 5 Average loss: 50.25799995  running time 20.20179796218872
====> Epoch: 5 Average loss: 50.03000000  running time 20.24186611175537
====> Epoch: 5 Average loss: 50.28200005  running time 20.127687692642212
====> Test set BCE loss 49.963993072509766 Custom Loss 49.963993072509766 with ber  0.4996400475502014
====> Epoch: 6 Average loss: 50.21599991  running time 20.197449207305908
====> Epoch: 6 Average loss: 50.14600006  running time 22.602754592895508
====> Epoch: 6 Average loss: 49.99000008  running time 22.69101905822754
====> Epoch: 6 Average loss: 49.96199997  running time 22.43220019340515
====> Epoch: 6 Average loss: 50.27799973  running time 22.605695724487305
====> Epoch: 6 Average loss: 50.30800011  running time 22.833295583724976
====> Epoch: 6 Average loss: 50.08799995  running time 20.316504955291748
====> Epoch: 6 Average loss: 50.16799980  running time 20.257641792297363
====> Epoch: 6 Average loss: 49.93200027  running time 20.62247896194458
====> Epoch: 6 Average loss: 49.28399971  running time 20.68149709701538
====> Epoch: 6 Average loss: 49.96399986  running time 20.48563814163208
====> Epoch: 6 Average loss: 49.92399994  running time 20.30536198616028
====> Test set BCE loss 50.3279914855957 Custom Loss 50.3279914855957 with ber  0.5032799243927002
====> Epoch: 7 Average loss: 50.20200020  running time 20.744962692260742
====> Epoch: 7 Average loss: 50.42799988  running time 22.747132301330566
====> Epoch: 7 Average loss: 50.11199997  running time 22.759002923965454
====> Epoch: 7 Average loss: 50.07000000  running time 23.56196355819702
====> Epoch: 7 Average loss: 50.02000008  running time 22.957971334457397
====> Epoch: 7 Average loss: 49.67400009  running time 22.808724880218506
====> Epoch: 7 Average loss: 49.87799995  running time 20.249009370803833
====> Epoch: 7 Average loss: 50.20199989  running time 20.347975730895996
====> Epoch: 7 Average loss: 50.10800011  running time 20.148235082626343
====> Epoch: 7 Average loss: 50.05399986  running time 20.269614458084106
====> Epoch: 7 Average loss: 49.57599976  running time 20.210408210754395
====> Epoch: 7 Average loss: 49.91999992  running time 20.215071201324463
====> Test set BCE loss 49.814002990722656 Custom Loss 49.814002990722656 with ber  0.4981399476528168
====> Epoch: 8 Average loss: 50.13800003  running time 20.33074951171875
====> Epoch: 8 Average loss: 50.19000023  running time 22.802457094192505
====> Epoch: 8 Average loss: 50.04599991  running time 22.784172296524048
====> Epoch: 8 Average loss: 49.91599968  running time 23.124546766281128
====> Epoch: 8 Average loss: 49.98400002  running time 23.153212308883667
====> Epoch: 8 Average loss: 49.79199997  running time 23.073432207107544
====> Epoch: 8 Average loss: 49.69800003  running time 21.198333024978638
====> Epoch: 8 Average loss: 50.00400017  running time 20.756136417388916
====> Epoch: 8 Average loss: 49.84399994  running time 20.576145887374878
====> Epoch: 8 Average loss: 50.04200005  running time 20.711058139801025
====> Epoch: 8 Average loss: 50.01999977  running time 20.699771642684937
====> Epoch: 8 Average loss: 49.79800026  running time 20.65424108505249
====> Test set BCE loss 50.34200668334961 Custom Loss 50.34200668334961 with ber  0.5034200549125671
====> Epoch: 9 Average loss: 49.83600021  running time 20.48636770248413
====> Epoch: 9 Average loss: 50.07199982  running time 23.364596128463745
====> Epoch: 9 Average loss: 50.13000023  running time 22.720290422439575
====> Epoch: 9 Average loss: 50.07200005  running time 22.90976309776306
====> Epoch: 9 Average loss: 49.95000008  running time 22.748611450195312
====> Epoch: 9 Average loss: 49.94400002  running time 23.838422775268555
====> Epoch: 9 Average loss: 50.06800026  running time 20.202471494674683
====> Epoch: 9 Average loss: 49.91599998  running time 20.49800753593445
====> Epoch: 9 Average loss: 49.99400002  running time 20.633562088012695
====> Epoch: 9 Average loss: 49.97799988  running time 20.273027420043945
====> Epoch: 9 Average loss: 49.88000031  running time 20.245933771133423
====> Epoch: 9 Average loss: 49.94600014  running time 21.250421047210693
====> Test set BCE loss 49.984004974365234 Custom Loss 49.984004974365234 with ber  0.49984002113342285
====> Epoch: 10 Average loss: 50.13600014  running time 20.45287585258484
====> Epoch: 10 Average loss: 50.09999985  running time 22.657698392868042
====> Epoch: 10 Average loss: 50.02199989  running time 22.732046365737915
====> Epoch: 10 Average loss: 50.21600021  running time 22.660621643066406
====> Epoch: 10 Average loss: 50.13599983  running time 22.87550139427185
====> Epoch: 10 Average loss: 50.10799988  running time 22.56568145751953
====> Epoch: 10 Average loss: 49.88400002  running time 20.285933017730713
====> Epoch: 10 Average loss: 50.08199997  running time 20.22527575492859
====> Epoch: 10 Average loss: 50.15799995  running time 20.300361156463623
====> Epoch: 10 Average loss: 50.12200005  running time 20.20590829849243
====> Epoch: 10 Average loss: 50.04999992  running time 20.582929849624634
====> Epoch: 10 Average loss: 49.60400002  running time 20.249149084091187
====> Test set BCE loss 49.805999755859375 Custom Loss 49.805999755859375 with ber  0.4980599284172058
saved model C:\WorkSpace\FadingChannels\Swetha_M20AIE317_MTP\Fading\20230726_035537\model_faded\bl_10__k_1_n_3_chl_fading_mod_QAM2\attention_model_10_fading_lr_0.01_D1bl_10__k_1_n_3_chl_fading_mod_QAM2_5000_20230726_035537.pt
SNRS [0.0, 5.0, 10.0, 15.0, 20.0, 25.0, 30.0, 35.0, 40.0, 45.0, 50.0, 55.0, 60.0, 65.0, 70.0, 75.0, 80.0, 85.0, 90.0, 95.0]
no pos BER specified.
Test SNR 0.0 with ber  0.5017399787902832 with bler 0.9990000000000001
Punctured Test SNR 0.0 with ber  0.0 with bler 0.0
no pos BER specified.
Test SNR 5.0 with ber  0.49921998381614685 with bler 0.9988000000000002
Punctured Test SNR 5.0 with ber  0.0 with bler 0.0
no pos BER specified.
Test SNR 10.0 with ber  0.5020400285720825 with bler 0.9994
Punctured Test SNR 10.0 with ber  0.0 with bler 0.0
no pos BER specified.
Test SNR 15.0 with ber  0.4978999197483063 with bler 0.9990000000000001
Punctured Test SNR 15.0 with ber  0.0 with bler 0.0
no pos BER specified.
Test SNR 20.0 with ber  0.49824008345603943 with bler 0.9992000000000002
Punctured Test SNR 20.0 with ber  0.0 with bler 0.0
no pos BER specified.
Test SNR 25.0 with ber  0.5028600096702576 with bler 0.9992
Punctured Test SNR 25.0 with ber  0.0 with bler 0.0
no pos BER specified.
Test SNR 30.0 with ber  0.5002999305725098 with bler 0.9987999999999999
Punctured Test SNR 30.0 with ber  0.0 with bler 0.0
no pos BER specified.
Test SNR 35.0 with ber  0.5023200511932373 with bler 0.9992
Punctured Test SNR 35.0 with ber  0.0 with bler 0.0
no pos BER specified.
Test SNR 40.0 with ber  0.5042399764060974 with bler 0.9987999999999999
Punctured Test SNR 40.0 with ber  0.0 with bler 0.0
no pos BER specified.
Test SNR 45.0 with ber  0.4993201196193695 with bler 0.9994000000000001
Punctured Test SNR 45.0 with ber  0.0 with bler 0.0
no pos BER specified.
Test SNR 50.0 with ber  0.5045999884605408 with bler 0.9995999999999999
Punctured Test SNR 50.0 with ber  0.0 with bler 0.0
no pos BER specified.
Test SNR 55.0 with ber  0.4950599670410156 with bler 0.9995999999999999
Punctured Test SNR 55.0 with ber  0.0 with bler 0.0
no pos BER specified.
Test SNR 60.0 with ber  0.5013201236724854 with bler 0.9994000000000001
Punctured Test SNR 60.0 with ber  0.0 with bler 0.0
no pos BER specified.
Test SNR 65.0 with ber  0.5006600618362427 with bler 0.9994
Punctured Test SNR 65.0 with ber  0.0 with bler 0.0
no pos BER specified.
Test SNR 70.0 with ber  0.49950000643730164 with bler 0.9994
Punctured Test SNR 70.0 with ber  0.0 with bler 0.0
no pos BER specified.
Test SNR 75.0 with ber  0.5034399628639221 with bler 0.9987999999999999
Punctured Test SNR 75.0 with ber  0.0 with bler 0.0
no pos BER specified.
Test SNR 80.0 with ber  0.49720001220703125 with bler 0.9992000000000002
Punctured Test SNR 80.0 with ber  0.0 with bler 0.0
no pos BER specified.
Test SNR 85.0 with ber  0.49915993213653564 with bler 0.9987999999999999
Punctured Test SNR 85.0 with ber  0.0 with bler 0.0
no pos BER specified.
Test SNR 90.0 with ber  0.49754002690315247 with bler 0.9994
Punctured Test SNR 90.0 with ber  0.0 with bler 0.0
no pos BER specified.
Test SNR 95.0 with ber  0.5008800625801086 with bler 0.9987999999999999
Punctured Test SNR 95.0 with ber  0.0 with bler 0.0
final results on SNRs  [0.0, 5.0, 10.0, 15.0, 20.0, 25.0, 30.0, 35.0, 40.0, 45.0, 50.0, 55.0, 60.0, 65.0, 70.0, 75.0, 80.0, 85.0, 90.0, 95.0]
BER [0.5017399787902832, 0.49921998381614685, 0.5020400285720825, 0.4978999197483063, 0.49824008345603943, 0.5028600096702576, 0.5002999305725098, 0.5023200511932373, 0.5042399764060974, 0.4993201196193695, 0.5045999884605408, 0.4950599670410156, 0.5013201236724854, 0.5006600618362427, 0.49950000643730164, 0.5034399628639221, 0.49720001220703125, 0.49915993213653564, 0.49754002690315247, 0.5008800625801086]
BLER [0.9990000000000001, 0.9988000000000002, 0.9994, 0.9990000000000001, 0.9992000000000002, 0.9992, 0.9987999999999999, 0.9992, 0.9987999999999999, 0.9994000000000001, 0.9995999999999999, 0.9995999999999999, 0.9994000000000001, 0.9994, 0.9994, 0.9987999999999999, 0.9992000000000002, 0.9987999999999999, 0.9994, 0.9987999999999999]
final results on punctured SNRs  [0.0, 5.0, 10.0, 15.0, 20.0, 25.0, 30.0, 35.0, 40.0, 45.0, 50.0, 55.0, 60.0, 65.0, 70.0, 75.0, 80.0, 85.0, 90.0, 95.0]
BER [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
BLER [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
encoder power is tensor(1.)
adjusted SNR should be [-0.0, 4.999999888090176, 10.00000005838476, 15.000000078774018, 19.999999870570157, 25.000000003171387, 30.000000467677864, 34.99999989681464, 40.00000019414476, 45.00000000317138, 50.000000083965574, 54.99999989681464, 59.99999958744239, 65.00000036280017, 70.00000024384569, 75.0000000389704, 80.00000021942404, 84.9999998008802, 89.99999964429526, 95.0000003943598]

 
 ###############################################################################################
Training Model for : block length => 20 coderate_k => 1 coderate_n => 3 channel => awgn mod type=> QAM2
using random interleaver [18  1 19  8 10 17  6 13  4  2  5 14  9  7 16 11  3  0 15 12] [11  1 18 17  2 12 19 16 10  0  3  4 15  8 13  9  5 14  7  6]
Channel_ModAE(
  (enc): ENC_interCNN(
    (enc_cnn_1): SameShapeConv1d(
      (cnns): ModuleList(
        (0): Conv1d(1, 25, kernel_size=(5,), stride=(1,), padding=(2,))
        (1): Conv1d(25, 25, kernel_size=(5,), stride=(1,), padding=(2,))
        (2): Conv1d(25, 25, kernel_size=(5,), stride=(1,), padding=(2,))
      )
    )
    (enc_cnn_2): SameShapeConv1d(
      (cnns): ModuleList(
        (0): Conv1d(1, 25, kernel_size=(5,), stride=(1,), padding=(2,))
        (1): Conv1d(25, 25, kernel_size=(5,), stride=(1,), padding=(2,))
        (2): Conv1d(25, 25, kernel_size=(5,), stride=(1,), padding=(2,))
      )
    )
    (enc_cnn_3): SameShapeConv1d(
      (cnns): ModuleList(
        (0): Conv1d(1, 25, kernel_size=(5,), stride=(1,), padding=(2,))
        (1): Conv1d(25, 25, kernel_size=(5,), stride=(1,), padding=(2,))
        (2): Conv1d(25, 25, kernel_size=(5,), stride=(1,), padding=(2,))
      )
    )
    (enc_linear_1): Linear(in_features=25, out_features=1, bias=True)
    (enc_linear_2): Linear(in_features=25, out_features=1, bias=True)
    (enc_linear_3): Linear(in_features=25, out_features=1, bias=True)
    (interleaver): Interleaver()
  )
  (dec): DEC_LargeCNN(
    (interleaver): Interleaver()
    (deinterleaver): DeInterleaver()
    (dec1_cnns): ModuleList(
      (0): SameShapeConv1d(
        (cnns): ModuleList(
          (0): Conv1d(7, 100, kernel_size=(5,), stride=(1,), padding=(2,))
          (1): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,))
          (2): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,))
        )
      )
      (1): SameShapeConv1d(
        (cnns): ModuleList(
          (0): Conv1d(7, 100, kernel_size=(5,), stride=(1,), padding=(2,))
          (1): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,))
          (2): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,))
        )
      )
      (2): SameShapeConv1d(
        (cnns): ModuleList(
          (0): Conv1d(7, 100, kernel_size=(5,), stride=(1,), padding=(2,))
          (1): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,))
          (2): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,))
        )
      )
      (3): SameShapeConv1d(
        (cnns): ModuleList(
          (0): Conv1d(7, 100, kernel_size=(5,), stride=(1,), padding=(2,))
          (1): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,))
          (2): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,))
        )
      )
      (4): SameShapeConv1d(
        (cnns): ModuleList(
          (0): Conv1d(7, 100, kernel_size=(5,), stride=(1,), padding=(2,))
          (1): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,))
          (2): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,))
        )
      )
      (5): SameShapeConv1d(
        (cnns): ModuleList(
          (0): Conv1d(7, 100, kernel_size=(5,), stride=(1,), padding=(2,))
          (1): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,))
          (2): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,))
        )
      )
    )
    (dec2_cnns): ModuleList(
      (0): SameShapeConv1d(
        (cnns): ModuleList(
          (0): Conv1d(7, 100, kernel_size=(5,), stride=(1,), padding=(2,))
          (1): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,))
          (2): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,))
        )
      )
      (1): SameShapeConv1d(
        (cnns): ModuleList(
          (0): Conv1d(7, 100, kernel_size=(5,), stride=(1,), padding=(2,))
          (1): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,))
          (2): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,))
        )
      )
      (2): SameShapeConv1d(
        (cnns): ModuleList(
          (0): Conv1d(7, 100, kernel_size=(5,), stride=(1,), padding=(2,))
          (1): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,))
          (2): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,))
        )
      )
      (3): SameShapeConv1d(
        (cnns): ModuleList(
          (0): Conv1d(7, 100, kernel_size=(5,), stride=(1,), padding=(2,))
          (1): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,))
          (2): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,))
        )
      )
      (4): SameShapeConv1d(
        (cnns): ModuleList(
          (0): Conv1d(7, 100, kernel_size=(5,), stride=(1,), padding=(2,))
          (1): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,))
          (2): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,))
        )
      )
      (5): SameShapeConv1d(
        (cnns): ModuleList(
          (0): Conv1d(7, 100, kernel_size=(5,), stride=(1,), padding=(2,))
          (1): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,))
          (2): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,))
        )
      )
    )
    (dec1_outputs): ModuleList(
      (0): Linear(in_features=100, out_features=5, bias=True)
      (1): Linear(in_features=100, out_features=5, bias=True)
      (2): Linear(in_features=100, out_features=5, bias=True)
      (3): Linear(in_features=100, out_features=5, bias=True)
      (4): Linear(in_features=100, out_features=5, bias=True)
      (5): Linear(in_features=100, out_features=5, bias=True)
    )
    (dec2_outputs): ModuleList(
      (0): Linear(in_features=100, out_features=5, bias=True)
      (1): Linear(in_features=100, out_features=5, bias=True)
      (2): Linear(in_features=100, out_features=5, bias=True)
      (3): Linear(in_features=100, out_features=5, bias=True)
      (4): Linear(in_features=100, out_features=5, bias=True)
      (5): Linear(in_features=100, out_features=1, bias=True)
    )
  )
  (mod): Modulation(
    (mod_layer): SameShapeConv1d(
      (cnns): ModuleList(
        (0): Conv1d(2, 20, kernel_size=(1,), stride=(1,))
      )
    )
    (mod_final): SameShapeConv1d(
      (cnns): ModuleList(
        (0): Conv1d(20, 2, kernel_size=(1,), stride=(1,))
      )
    )
  )
  (demod): DeModulation(
    (demod_layer): SameShapeConv1d(
      (cnns): ModuleList(
        (0): Conv1d(2, 20, kernel_size=(1,), stride=(1,))
      )
    )
    (demod_final): SameShapeConv1d(
      (cnns): ModuleList(
        (0): Conv1d(20, 2, kernel_size=(1,), stride=(1,))
      )
    )
  )
)
====> Epoch: 1 Average loss: 0.69007039  running time 22.201792001724243
====> Epoch: 1 Average loss: 46.90342870  running time 35.56608438491821
====> Epoch: 1 Average loss: 49.98199997  running time 36.1150643825531
====> Epoch: 1 Average loss: 50.03000000  running time 35.9132936000824
====> Epoch: 1 Average loss: 50.00900009  running time 35.934916496276855
====> Epoch: 1 Average loss: 50.13000008  running time 35.81925106048584
====> Epoch: 1 Average loss: 49.94399986  running time 35.17866659164429
====> Epoch: 1 Average loss: 49.91700020  running time 35.325090169906616
====> Epoch: 1 Average loss: 49.96500008  running time 35.36995720863342
====> Epoch: 1 Average loss: 50.14699997  running time 35.06264638900757
====> Epoch: 1 Average loss: 50.00099991  running time 35.13989019393921
====> Epoch: 1 Average loss: 50.15300011  running time 35.15908694267273
====> Test set BCE loss 50.16600036621094 Custom Loss 50.16600036621094 with ber  0.5016598701477051
====> Epoch: 2 Average loss: 50.08599991  running time 35.15501689910889
====> Epoch: 2 Average loss: 50.12600006  running time 36.725462675094604
====> Epoch: 2 Average loss: 49.89700027  running time 35.97294068336487
====> Epoch: 2 Average loss: 49.80299995  running time 36.09027576446533
====> Epoch: 2 Average loss: 49.94799995  running time 36.007568359375
====> Epoch: 2 Average loss: 49.97899994  running time 36.03847789764404
====> Epoch: 2 Average loss: 49.92600021  running time 35.17111420631409
====> Epoch: 2 Average loss: 49.93699982  running time 35.45070552825928
====> Epoch: 2 Average loss: 50.06200012  running time 35.26479768753052
====> Epoch: 2 Average loss: 49.87099991  running time 35.36176514625549
====> Epoch: 2 Average loss: 49.94499992  running time 34.95938444137573
====> Epoch: 2 Average loss: 49.72199997  running time 40.177061319351196
====> Test set BCE loss 50.06300354003906 Custom Loss 50.06300354003906 with ber  0.5006300806999207
====> Epoch: 3 Average loss: 50.02399986  running time 35.774303913116455
====> Epoch: 3 Average loss: 49.99699974  running time 36.32246732711792
====> Epoch: 3 Average loss: 50.00899994  running time 36.223814964294434
====> Epoch: 3 Average loss: 49.78999985  running time 36.125896692276
====> Epoch: 3 Average loss: 50.20200005  running time 35.97763252258301
====> Epoch: 3 Average loss: 50.04199997  running time 37.27430987358093
====> Epoch: 3 Average loss: 50.35299988  running time 35.1934130191803
====> Epoch: 3 Average loss: 49.98499985  running time 35.317485332489014
====> Epoch: 3 Average loss: 49.94900002  running time 35.3737256526947
====> Epoch: 3 Average loss: 49.94400017  running time 35.3562798500061
====> Epoch: 3 Average loss: 50.04399994  running time 36.186310052871704
====> Epoch: 3 Average loss: 50.14900002  running time 35.26396822929382
====> Test set BCE loss 49.981998443603516 Custom Loss 49.981998443603516 with ber  0.49981993436813354
====> Epoch: 4 Average loss: 49.98999969  running time 35.371681928634644
====> Epoch: 4 Average loss: 49.85299995  running time 38.72518444061279
====> Epoch: 4 Average loss: 50.06300018  running time 37.6759467124939
====> Epoch: 4 Average loss: 49.87800003  running time 38.13586688041687
====> Epoch: 4 Average loss: 49.90599983  running time 37.4248526096344
====> Epoch: 4 Average loss: 50.13399994  running time 37.69000768661499
====> Epoch: 4 Average loss: 50.10099968  running time 35.04905581474304
====> Epoch: 4 Average loss: 49.97300003  running time 35.238933801651
====> Epoch: 4 Average loss: 50.14500023  running time 35.151686668395996
====> Epoch: 4 Average loss: 49.96700035  running time 35.22752118110657
====> Epoch: 4 Average loss: 50.09400002  running time 35.27086663246155
====> Epoch: 4 Average loss: 49.93599998  running time 35.05721926689148
====> Test set BCE loss 50.011009216308594 Custom Loss 50.011009216308594 with ber  0.5001099109649658
====> Epoch: 5 Average loss: 50.05500023  running time 35.2332649230957
====> Epoch: 5 Average loss: 49.79800026  running time 37.469335079193115
====> Epoch: 5 Average loss: 50.08500000  running time 37.87961292266846
====> Epoch: 5 Average loss: 50.04799988  running time 37.694018840789795
====> Epoch: 5 Average loss: 50.04199989  running time 37.821579456329346
====> Epoch: 5 Average loss: 49.88999992  running time 37.279088258743286
====> Epoch: 5 Average loss: 49.63199997  running time 35.35732316970825
====> Epoch: 5 Average loss: 49.90100014  running time 35.478535413742065
====> Epoch: 5 Average loss: 49.98099991  running time 35.19103717803955
====> Epoch: 5 Average loss: 50.10099976  running time 35.17726397514343
====> Epoch: 5 Average loss: 49.78100006  running time 35.329508781433105
====> Epoch: 5 Average loss: 50.27000008  running time 35.19470477104187
====> Test set BCE loss 50.0629997253418 Custom Loss 50.0629997253418 with ber  0.5006299614906311
====> Epoch: 6 Average loss: 49.81600006  running time 35.236183166503906
====> Epoch: 6 Average loss: 50.21499992  running time 38.513408184051514
====> Epoch: 6 Average loss: 49.68599998  running time 37.58698558807373
====> Epoch: 6 Average loss: 50.17099998  running time 37.67441749572754
====> Epoch: 6 Average loss: 50.05800018  running time 38.01259183883667
====> Epoch: 6 Average loss: 49.95200035  running time 37.5350136756897
====> Epoch: 6 Average loss: 49.96700012  running time 35.037710189819336
====> Epoch: 6 Average loss: 50.07999977  running time 35.27243089675903
====> Epoch: 6 Average loss: 50.18099983  running time 35.15525794029236
====> Epoch: 6 Average loss: 50.16800003  running time 36.0362024307251
====> Epoch: 6 Average loss: 49.99000000  running time 35.02937984466553
====> Epoch: 6 Average loss: 50.06800003  running time 35.316375732421875
====> Test set BCE loss 49.86299133300781 Custom Loss 49.86299133300781 with ber  0.49862995743751526
====> Epoch: 7 Average loss: 50.48399979  running time 35.37825846672058
====> Epoch: 7 Average loss: 49.91399994  running time 37.524866819381714
====> Epoch: 7 Average loss: 50.23499985  running time 37.64987587928772
====> Epoch: 7 Average loss: 49.74099968  running time 37.71315860748291
====> Epoch: 7 Average loss: 49.94800018  running time 37.63114285469055
====> Epoch: 7 Average loss: 49.88599991  running time 37.9292528629303
====> Epoch: 7 Average loss: 49.74900017  running time 35.22629451751709
====> Epoch: 7 Average loss: 50.04900002  running time 35.45357155799866
====> Epoch: 7 Average loss: 49.78199997  running time 35.8227002620697
====> Epoch: 7 Average loss: 49.92599998  running time 35.2356333732605
====> Epoch: 7 Average loss: 49.82600014  running time 36.9675407409668
====> Epoch: 7 Average loss: 49.85199974  running time 35.083786487579346
====> Test set BCE loss 50.1609992980957 Custom Loss 50.1609992980957 with ber  0.5016101002693176
====> Epoch: 8 Average loss: 50.06299988  running time 34.97386932373047
====> Epoch: 8 Average loss: 49.79500015  running time 37.665807485580444
====> Epoch: 8 Average loss: 50.06299995  running time 37.43334650993347
====> Epoch: 8 Average loss: 49.97900009  running time 37.48502063751221
====> Epoch: 8 Average loss: 50.32599998  running time 38.21820020675659
====> Epoch: 8 Average loss: 50.06099983  running time 38.063456296920776
====> Epoch: 8 Average loss: 50.20100006  running time 35.024176836013794
====> Epoch: 8 Average loss: 49.84299965  running time 35.73343062400818
====> Epoch: 8 Average loss: 49.94799988  running time 34.88884115219116
====> Epoch: 8 Average loss: 50.03800018  running time 34.80281662940979
====> Epoch: 8 Average loss: 49.94900002  running time 35.146963357925415
====> Epoch: 8 Average loss: 50.09499985  running time 34.90769648551941
====> Test set BCE loss 49.95600891113281 Custom Loss 49.95600891113281 with ber  0.4995599687099457
====> Epoch: 9 Average loss: 50.21299980  running time 35.05743622779846
====> Epoch: 9 Average loss: 50.00199982  running time 37.83065724372864
====> Epoch: 9 Average loss: 49.93799980  running time 37.57328915596008
====> Epoch: 9 Average loss: 49.76700012  running time 37.647629737854004
====> Epoch: 9 Average loss: 50.36000008  running time 37.75504517555237
====> Epoch: 9 Average loss: 49.79699989  running time 37.59121656417847
====> Epoch: 9 Average loss: 49.82799988  running time 35.16198253631592
====> Epoch: 9 Average loss: 49.76499992  running time 35.133800745010376
====> Epoch: 9 Average loss: 49.92299995  running time 35.28904843330383
====> Epoch: 9 Average loss: 50.14500000  running time 35.26734638214111
====> Epoch: 9 Average loss: 49.96000015  running time 36.46692657470703
====> Epoch: 9 Average loss: 50.13499985  running time 34.88726782798767
====> Test set BCE loss 50.207000732421875 Custom Loss 50.207000732421875 with ber  0.5020700097084045
====> Epoch: 10 Average loss: 50.07299980  running time 35.00960612297058
====> Epoch: 10 Average loss: 49.97099991  running time 37.326902627944946
====> Epoch: 10 Average loss: 49.86599976  running time 37.40576672554016
====> Epoch: 10 Average loss: 50.18100029  running time 37.439457178115845
====> Epoch: 10 Average loss: 49.88399986  running time 37.44677805900574
====> Epoch: 10 Average loss: 49.85199982  running time 38.74299240112305
====> Epoch: 10 Average loss: 49.89700005  running time 34.906593799591064
====> Epoch: 10 Average loss: 50.04299988  running time 34.81179332733154
====> Epoch: 10 Average loss: 49.88500000  running time 35.14563465118408
====> Epoch: 10 Average loss: 50.15099998  running time 34.882333278656006
====> Epoch: 10 Average loss: 50.09400002  running time 34.8481502532959
====> Epoch: 10 Average loss: 49.90499992  running time 34.834534883499146
====> Test set BCE loss 49.946998596191406 Custom Loss 49.946998596191406 with ber  0.4994700253009796
saved model C:\WorkSpace\FadingChannels\Swetha_M20AIE317_MTP\Fading\20230726_035537\model_faded\bl_20__k_1_n_3_chl_awgn_mod_QAM2\attention_model_10_awgn_lr_0.01_D1bl_20__k_1_n_3_chl_awgn_mod_QAM2_5000_20230726_035537.pt
SNRS [0.0, 5.0, 10.0, 15.0, 20.0, 25.0, 30.0, 35.0, 40.0, 45.0, 50.0, 55.0, 60.0, 65.0, 70.0, 75.0, 80.0, 85.0, 90.0, 95.0]
no pos BER specified.
Test SNR 0.0 with ber  0.5020399689674377 with bler 1.0
Punctured Test SNR 0.0 with ber  0.0 with bler 0.0
no pos BER specified.
Test SNR 5.0 with ber  0.49981987476348877 with bler 1.0
Punctured Test SNR 5.0 with ber  0.0 with bler 0.0
no pos BER specified.
Test SNR 10.0 with ber  0.5015300512313843 with bler 1.0
Punctured Test SNR 10.0 with ber  0.0 with bler 0.0
no pos BER specified.
Test SNR 15.0 with ber  0.4996199905872345 with bler 1.0
Punctured Test SNR 15.0 with ber  0.0 with bler 0.0
no pos BER specified.
Test SNR 20.0 with ber  0.5001100301742554 with bler 1.0
Punctured Test SNR 20.0 with ber  0.0 with bler 0.0
no pos BER specified.
Test SNR 25.0 with ber  0.5010699033737183 with bler 1.0
Punctured Test SNR 25.0 with ber  0.0 with bler 0.0
no pos BER specified.
Test SNR 30.0 with ber  0.49807995557785034 with bler 1.0
Punctured Test SNR 30.0 with ber  0.0 with bler 0.0
no pos BER specified.
Test SNR 35.0 with ber  0.5006300210952759 with bler 1.0
Punctured Test SNR 35.0 with ber  0.0 with bler 0.0
no pos BER specified.
Test SNR 40.0 with ber  0.5008100271224976 with bler 1.0
Punctured Test SNR 40.0 with ber  0.0 with bler 0.0
no pos BER specified.
Test SNR 45.0 with ber  0.5025099515914917 with bler 1.0
Punctured Test SNR 45.0 with ber  0.0 with bler 0.0
no pos BER specified.
Test SNR 50.0 with ber  0.49973994493484497 with bler 1.0
Punctured Test SNR 50.0 with ber  0.0 with bler 0.0
no pos BER specified.
Test SNR 55.0 with ber  0.5011500120162964 with bler 1.0
Punctured Test SNR 55.0 with ber  0.0 with bler 0.0
no pos BER specified.
Test SNR 60.0 with ber  0.5014600157737732 with bler 1.0
Punctured Test SNR 60.0 with ber  0.0 with bler 0.0
no pos BER specified.
Test SNR 65.0 with ber  0.4991600513458252 with bler 1.0
Punctured Test SNR 65.0 with ber  0.0 with bler 0.0
no pos BER specified.
Test SNR 70.0 with ber  0.5013700127601624 with bler 1.0
Punctured Test SNR 70.0 with ber  0.0 with bler 0.0
no pos BER specified.
Test SNR 75.0 with ber  0.5035799741744995 with bler 1.0
Punctured Test SNR 75.0 with ber  0.0 with bler 0.0
no pos BER specified.
Test SNR 80.0 with ber  0.4996699392795563 with bler 1.0
Punctured Test SNR 80.0 with ber  0.0 with bler 0.0
no pos BER specified.
Test SNR 85.0 with ber  0.4999101161956787 with bler 1.0
Punctured Test SNR 85.0 with ber  0.0 with bler 0.0
no pos BER specified.
Test SNR 90.0 with ber  0.4995300769805908 with bler 1.0
Punctured Test SNR 90.0 with ber  0.0 with bler 0.0
no pos BER specified.
Test SNR 95.0 with ber  0.4965200126171112 with bler 1.0
Punctured Test SNR 95.0 with ber  0.0 with bler 0.0
final results on SNRs  [0.0, 5.0, 10.0, 15.0, 20.0, 25.0, 30.0, 35.0, 40.0, 45.0, 50.0, 55.0, 60.0, 65.0, 70.0, 75.0, 80.0, 85.0, 90.0, 95.0]
BER [0.5020399689674377, 0.49981987476348877, 0.5015300512313843, 0.4996199905872345, 0.5001100301742554, 0.5010699033737183, 0.49807995557785034, 0.5006300210952759, 0.5008100271224976, 0.5025099515914917, 0.49973994493484497, 0.5011500120162964, 0.5014600157737732, 0.4991600513458252, 0.5013700127601624, 0.5035799741744995, 0.4996699392795563, 0.4999101161956787, 0.4995300769805908, 0.4965200126171112]
BLER [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]
final results on punctured SNRs  [0.0, 5.0, 10.0, 15.0, 20.0, 25.0, 30.0, 35.0, 40.0, 45.0, 50.0, 55.0, 60.0, 65.0, 70.0, 75.0, 80.0, 85.0, 90.0, 95.0]
BER [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
BLER [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
encoder power is tensor(1.)
adjusted SNR should be [-0.0, 4.999999888090176, 10.00000005838476, 15.000000078774018, 19.999999870570157, 25.000000003171387, 30.000000467677864, 34.99999989681464, 40.00000019414476, 45.00000000317138, 50.000000083965574, 54.99999989681464, 59.99999958744239, 65.00000036280017, 70.00000024384569, 75.0000000389704, 80.00000021942404, 84.9999998008802, 89.99999964429526, 95.0000003943598]

 
 ###############################################################################################
Training Model for : block length => 20 coderate_k => 1 coderate_n => 3 channel => fading mod type=> QAM2
using random interleaver [18  1 19  8 10 17  6 13  4  2  5 14  9  7 16 11  3  0 15 12] [11  1 18 17  2 12 19 16 10  0  3  4 15  8 13  9  5 14  7  6]
Channel_ModAE(
  (enc): ENC_interCNN(
    (enc_cnn_1): SameShapeConv1d(
      (cnns): ModuleList(
        (0): Conv1d(1, 25, kernel_size=(5,), stride=(1,), padding=(2,))
        (1): Conv1d(25, 25, kernel_size=(5,), stride=(1,), padding=(2,))
        (2): Conv1d(25, 25, kernel_size=(5,), stride=(1,), padding=(2,))
      )
    )
    (enc_cnn_2): SameShapeConv1d(
      (cnns): ModuleList(
        (0): Conv1d(1, 25, kernel_size=(5,), stride=(1,), padding=(2,))
        (1): Conv1d(25, 25, kernel_size=(5,), stride=(1,), padding=(2,))
        (2): Conv1d(25, 25, kernel_size=(5,), stride=(1,), padding=(2,))
      )
    )
    (enc_cnn_3): SameShapeConv1d(
      (cnns): ModuleList(
        (0): Conv1d(1, 25, kernel_size=(5,), stride=(1,), padding=(2,))
        (1): Conv1d(25, 25, kernel_size=(5,), stride=(1,), padding=(2,))
        (2): Conv1d(25, 25, kernel_size=(5,), stride=(1,), padding=(2,))
      )
    )
    (enc_linear_1): Linear(in_features=25, out_features=1, bias=True)
    (enc_linear_2): Linear(in_features=25, out_features=1, bias=True)
    (enc_linear_3): Linear(in_features=25, out_features=1, bias=True)
    (interleaver): Interleaver()
  )
  (dec): DEC_LargeCNN(
    (interleaver): Interleaver()
    (deinterleaver): DeInterleaver()
    (dec1_cnns): ModuleList(
      (0): SameShapeConv1d(
        (cnns): ModuleList(
          (0): Conv1d(7, 100, kernel_size=(5,), stride=(1,), padding=(2,))
          (1): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,))
          (2): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,))
        )
      )
      (1): SameShapeConv1d(
        (cnns): ModuleList(
          (0): Conv1d(7, 100, kernel_size=(5,), stride=(1,), padding=(2,))
          (1): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,))
          (2): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,))
        )
      )
      (2): SameShapeConv1d(
        (cnns): ModuleList(
          (0): Conv1d(7, 100, kernel_size=(5,), stride=(1,), padding=(2,))
          (1): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,))
          (2): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,))
        )
      )
      (3): SameShapeConv1d(
        (cnns): ModuleList(
          (0): Conv1d(7, 100, kernel_size=(5,), stride=(1,), padding=(2,))
          (1): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,))
          (2): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,))
        )
      )
      (4): SameShapeConv1d(
        (cnns): ModuleList(
          (0): Conv1d(7, 100, kernel_size=(5,), stride=(1,), padding=(2,))
          (1): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,))
          (2): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,))
        )
      )
      (5): SameShapeConv1d(
        (cnns): ModuleList(
          (0): Conv1d(7, 100, kernel_size=(5,), stride=(1,), padding=(2,))
          (1): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,))
          (2): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,))
        )
      )
    )
    (dec2_cnns): ModuleList(
      (0): SameShapeConv1d(
        (cnns): ModuleList(
          (0): Conv1d(7, 100, kernel_size=(5,), stride=(1,), padding=(2,))
          (1): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,))
          (2): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,))
        )
      )
      (1): SameShapeConv1d(
        (cnns): ModuleList(
          (0): Conv1d(7, 100, kernel_size=(5,), stride=(1,), padding=(2,))
          (1): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,))
          (2): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,))
        )
      )
      (2): SameShapeConv1d(
        (cnns): ModuleList(
          (0): Conv1d(7, 100, kernel_size=(5,), stride=(1,), padding=(2,))
          (1): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,))
          (2): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,))
        )
      )
      (3): SameShapeConv1d(
        (cnns): ModuleList(
          (0): Conv1d(7, 100, kernel_size=(5,), stride=(1,), padding=(2,))
          (1): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,))
          (2): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,))
        )
      )
      (4): SameShapeConv1d(
        (cnns): ModuleList(
          (0): Conv1d(7, 100, kernel_size=(5,), stride=(1,), padding=(2,))
          (1): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,))
          (2): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,))
        )
      )
      (5): SameShapeConv1d(
        (cnns): ModuleList(
          (0): Conv1d(7, 100, kernel_size=(5,), stride=(1,), padding=(2,))
          (1): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,))
          (2): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,))
        )
      )
    )
    (dec1_outputs): ModuleList(
      (0): Linear(in_features=100, out_features=5, bias=True)
      (1): Linear(in_features=100, out_features=5, bias=True)
      (2): Linear(in_features=100, out_features=5, bias=True)
      (3): Linear(in_features=100, out_features=5, bias=True)
      (4): Linear(in_features=100, out_features=5, bias=True)
      (5): Linear(in_features=100, out_features=5, bias=True)
    )
    (dec2_outputs): ModuleList(
      (0): Linear(in_features=100, out_features=5, bias=True)
      (1): Linear(in_features=100, out_features=5, bias=True)
      (2): Linear(in_features=100, out_features=5, bias=True)
      (3): Linear(in_features=100, out_features=5, bias=True)
      (4): Linear(in_features=100, out_features=5, bias=True)
      (5): Linear(in_features=100, out_features=1, bias=True)
    )
  )
  (mod): Modulation(
    (mod_layer): SameShapeConv1d(
      (cnns): ModuleList(
        (0): Conv1d(2, 20, kernel_size=(1,), stride=(1,))
      )
    )
    (mod_final): SameShapeConv1d(
      (cnns): ModuleList(
        (0): Conv1d(20, 2, kernel_size=(1,), stride=(1,))
      )
    )
  )
  (demod): DeModulation(
    (demod_layer): SameShapeConv1d(
      (cnns): ModuleList(
        (0): Conv1d(2, 20, kernel_size=(1,), stride=(1,))
      )
    )
    (demod_final): SameShapeConv1d(
      (cnns): ModuleList(
        (0): Conv1d(20, 2, kernel_size=(1,), stride=(1,))
      )
    )
  )
)
====> Epoch: 1 Average loss: 0.69244750  running time 22.625378370285034
====> Epoch: 1 Average loss: 46.13935462  running time 33.79470181465149
====> Epoch: 1 Average loss: 50.06699982  running time 34.859676361083984
====> Epoch: 1 Average loss: 50.27099998  running time 35.487274408340454
====> Epoch: 1 Average loss: 50.00199997  running time 34.33365869522095
====> Epoch: 1 Average loss: 50.10299988  running time 34.371145725250244
====> Epoch: 1 Average loss: 49.73800003  running time 33.59298086166382
====> Epoch: 1 Average loss: 49.90799980  running time 34.63013982772827
====> Epoch: 1 Average loss: 50.02499992  running time 33.59046244621277
====> Epoch: 1 Average loss: 50.17000008  running time 33.54971361160278
====> Epoch: 1 Average loss: 49.77800011  running time 33.13369607925415
====> Epoch: 1 Average loss: 50.04200005  running time 33.60509753227234
====> Test set BCE loss 49.96500015258789 Custom Loss 49.96500015258789 with ber  0.4996500313282013
====> Epoch: 2 Average loss: 50.04299980  running time 33.46305584907532
====> Epoch: 2 Average loss: 50.09200005  running time 34.330092430114746
====> Epoch: 2 Average loss: 49.83599991  running time 34.34683632850647
====> Epoch: 2 Average loss: 50.00900017  running time 34.346630334854126
====> Epoch: 2 Average loss: 49.86000000  running time 34.53262639045715
====> Epoch: 2 Average loss: 50.02500015  running time 34.70946526527405
====> Epoch: 2 Average loss: 49.98700012  running time 33.685837268829346
====> Epoch: 2 Average loss: 49.98199974  running time 33.46677112579346
====> Epoch: 2 Average loss: 49.98199982  running time 33.49421048164368
====> Epoch: 2 Average loss: 49.95700005  running time 33.546746492385864
====> Epoch: 2 Average loss: 50.06099983  running time 33.50357222557068
====> Epoch: 2 Average loss: 49.93200005  running time 33.95547294616699
====> Test set BCE loss 49.88900375366211 Custom Loss 49.88900375366211 with ber  0.4988900423049927
====> Epoch: 3 Average loss: 49.94000015  running time 34.620394468307495
====> Epoch: 3 Average loss: 49.73199982  running time 35.42479062080383
====> Epoch: 3 Average loss: 49.66499977  running time 35.461812257766724
====> Epoch: 3 Average loss: 50.30499992  running time 35.00091218948364
====> Epoch: 3 Average loss: 50.08100029  running time 34.98325061798096
====> Epoch: 3 Average loss: 49.98599991  running time 35.22266435623169
====> Epoch: 3 Average loss: 50.01599983  running time 33.583579540252686
====> Epoch: 3 Average loss: 50.02100006  running time 33.76194882392883
====> Epoch: 3 Average loss: 49.88000000  running time 33.79909944534302
====> Epoch: 3 Average loss: 50.09800011  running time 33.67838788032532
====> Epoch: 3 Average loss: 49.95199997  running time 33.20504140853882
====> Epoch: 3 Average loss: 50.02599998  running time 33.57026028633118
====> Test set BCE loss 49.966007232666016 Custom Loss 49.966007232666016 with ber  0.49966004490852356
====> Epoch: 4 Average loss: 49.87099991  running time 33.45376801490784
====> Epoch: 4 Average loss: 50.04099991  running time 37.10162329673767
====> Epoch: 4 Average loss: 50.05400009  running time 36.048566818237305
====> Epoch: 4 Average loss: 50.10400017  running time 36.95989775657654
====> Epoch: 4 Average loss: 49.97399979  running time 37.2830855846405
====> Epoch: 4 Average loss: 50.04700012  running time 36.63775300979614
====> Epoch: 4 Average loss: 50.11200012  running time 34.4550895690918
====> Epoch: 4 Average loss: 49.86000008  running time 34.30188274383545
====> Epoch: 4 Average loss: 49.90699982  running time 33.83871245384216
====> Epoch: 4 Average loss: 49.76600014  running time 33.73747253417969
====> Epoch: 4 Average loss: 50.14400009  running time 34.509095668792725
====> Epoch: 4 Average loss: 49.95800003  running time 34.40642738342285
====> Test set BCE loss 50.00299835205078 Custom Loss 50.00299835205078 with ber  0.5000298619270325
====> Epoch: 5 Average loss: 49.71900002  running time 34.74282741546631
====> Epoch: 5 Average loss: 50.26500008  running time 36.81358194351196
====> Epoch: 5 Average loss: 49.88400002  running time 36.72858738899231
====> Epoch: 5 Average loss: 49.80199982  running time 39.35492968559265
====> Epoch: 5 Average loss: 50.11800003  running time 36.68613648414612
====> Epoch: 5 Average loss: 50.15100014  running time 36.72209692001343
====> Epoch: 5 Average loss: 49.91900024  running time 34.75325107574463
====> Epoch: 5 Average loss: 49.81799988  running time 33.60737729072571
====> Epoch: 5 Average loss: 49.80500000  running time 33.87202453613281
====> Epoch: 5 Average loss: 50.06899979  running time 33.485718965530396
====> Epoch: 5 Average loss: 49.91499992  running time 33.4990336894989
====> Epoch: 5 Average loss: 50.00000008  running time 34.70208811759949
====> Test set BCE loss 50.0420036315918 Custom Loss 50.0420036315918 with ber  0.5004199147224426
====> Epoch: 6 Average loss: 49.97900009  running time 33.720924854278564
====> Epoch: 6 Average loss: 49.70700012  running time 36.18926930427551
====> Epoch: 6 Average loss: 50.16600014  running time 36.07523202896118
====> Epoch: 6 Average loss: 50.00599998  running time 37.03260540962219
====> Epoch: 6 Average loss: 50.01899994  running time 36.364742279052734
====> Epoch: 6 Average loss: 50.04700012  running time 35.805925369262695
====> Epoch: 6 Average loss: 50.11100006  running time 34.88018536567688
====> Epoch: 6 Average loss: 50.13899971  running time 33.59807109832764
====> Epoch: 6 Average loss: 50.23500008  running time 33.43586826324463
====> Epoch: 6 Average loss: 49.83699997  running time 33.54603958129883
====> Epoch: 6 Average loss: 50.07699982  running time 33.822409868240356
====> Epoch: 6 Average loss: 50.14400002  running time 33.68143439292908
====> Test set BCE loss 50.07499694824219 Custom Loss 50.07499694824219 with ber  0.500749945640564
====> Epoch: 7 Average loss: 49.92800003  running time 33.4319794178009
====> Epoch: 7 Average loss: 50.12000015  running time 35.95257329940796
====> Epoch: 7 Average loss: 50.12000000  running time 35.90304923057556
====> Epoch: 7 Average loss: 50.00200005  running time 35.97628831863403
====> Epoch: 7 Average loss: 50.16000000  running time 35.92951321601868
====> Epoch: 7 Average loss: 50.18100014  running time 35.887582302093506
====> Epoch: 7 Average loss: 49.89599983  running time 33.671557903289795
====> Epoch: 7 Average loss: 49.70500015  running time 33.41697406768799
====> Epoch: 7 Average loss: 49.98200020  running time 33.61516499519348
====> Epoch: 7 Average loss: 50.03200005  running time 33.269821882247925
====> Epoch: 7 Average loss: 50.21200005  running time 33.28961896896362
====> Epoch: 7 Average loss: 49.72300003  running time 33.50326228141785
====> Test set BCE loss 49.909000396728516 Custom Loss 49.909000396728516 with ber  0.4990900158882141
====> Epoch: 8 Average loss: 49.65700020  running time 33.5316641330719
====> Epoch: 8 Average loss: 49.94900009  running time 35.74943137168884
====> Epoch: 8 Average loss: 49.93400017  running time 36.32006549835205
====> Epoch: 8 Average loss: 49.81899994  running time 35.896196603775024
====> Epoch: 8 Average loss: 49.85799995  running time 35.937424659729004
====> Epoch: 8 Average loss: 49.98599968  running time 36.56684756278992
====> Epoch: 8 Average loss: 49.62900002  running time 33.570475339889526
====> Epoch: 8 Average loss: 50.00000015  running time 33.49831414222717
====> Epoch: 8 Average loss: 50.07899986  running time 33.31192064285278
====> Epoch: 8 Average loss: 50.08099991  running time 33.430529832839966
====> Epoch: 8 Average loss: 49.83999977  running time 33.35104775428772
====> Epoch: 8 Average loss: 49.87300003  running time 33.12396478652954
====> Test set BCE loss 49.88398742675781 Custom Loss 49.88398742675781 with ber  0.49883997440338135
====> Epoch: 9 Average loss: 49.94399986  running time 33.63529586791992
====> Epoch: 9 Average loss: 49.85199982  running time 37.44194436073303
====> Epoch: 9 Average loss: 49.97599991  running time 40.60728359222412
====> Epoch: 9 Average loss: 49.76400002  running time 39.55671715736389
====> Epoch: 9 Average loss: 49.97500000  running time 36.17200446128845
====> Epoch: 9 Average loss: 49.85400002  running time 36.613839864730835
====> Epoch: 9 Average loss: 50.04799965  running time 33.748636960983276
====> Epoch: 9 Average loss: 50.26400002  running time 34.36957025527954
====> Epoch: 9 Average loss: 49.86099983  running time 33.7884840965271
====> Epoch: 9 Average loss: 49.76599998  running time 34.69885778427124
====> Epoch: 9 Average loss: 50.12799957  running time 35.310895919799805
====> Epoch: 9 Average loss: 50.26300011  running time 33.628010749816895
====> Test set BCE loss 50.024993896484375 Custom Loss 50.024993896484375 with ber  0.500249981880188
====> Epoch: 10 Average loss: 50.04500015  running time 34.24352693557739
====> Epoch: 10 Average loss: 50.13000031  running time 36.24102282524109
====> Epoch: 10 Average loss: 50.10999992  running time 36.05520749092102
====> Epoch: 10 Average loss: 49.92200012  running time 36.59248900413513
====> Epoch: 10 Average loss: 50.14299995  running time 36.154717206954956
====> Epoch: 10 Average loss: 50.18199997  running time 36.43864679336548
====> Epoch: 10 Average loss: 49.90499992  running time 33.68818712234497
====> Epoch: 10 Average loss: 49.95999985  running time 33.96515226364136
====> Epoch: 10 Average loss: 50.06399979  running time 33.94260501861572
====> Epoch: 10 Average loss: 49.96299988  running time 33.73706650733948
====> Epoch: 10 Average loss: 50.42900002  running time 33.76000475883484
====> Epoch: 10 Average loss: 49.95399979  running time 34.9821195602417
====> Test set BCE loss 50.02300262451172 Custom Loss 50.02300262451172 with ber  0.5002298951148987
saved model C:\WorkSpace\FadingChannels\Swetha_M20AIE317_MTP\Fading\20230726_035537\model_faded\bl_20__k_1_n_3_chl_fading_mod_QAM2\attention_model_10_fading_lr_0.01_D1bl_20__k_1_n_3_chl_fading_mod_QAM2_5000_20230726_035537.pt
SNRS [0.0, 5.0, 10.0, 15.0, 20.0, 25.0, 30.0, 35.0, 40.0, 45.0, 50.0, 55.0, 60.0, 65.0, 70.0, 75.0, 80.0, 85.0, 90.0, 95.0]
no pos BER specified.
Test SNR 0.0 with ber  0.49999985098838806 with bler 1.0
Punctured Test SNR 0.0 with ber  0.0 with bler 0.0
no pos BER specified.
Test SNR 5.0 with ber  0.49772998690605164 with bler 1.0
Punctured Test SNR 5.0 with ber  0.0 with bler 0.0
no pos BER specified.
Test SNR 10.0 with ber  0.4984799921512604 with bler 1.0
Punctured Test SNR 10.0 with ber  0.0 with bler 0.0
no pos BER specified.
Test SNR 15.0 with ber  0.5013200640678406 with bler 1.0
Punctured Test SNR 15.0 with ber  0.0 with bler 0.0
no pos BER specified.
Test SNR 20.0 with ber  0.5015000700950623 with bler 1.0
Punctured Test SNR 20.0 with ber  0.0 with bler 0.0
no pos BER specified.
Test SNR 25.0 with ber  0.4983200430870056 with bler 1.0
Punctured Test SNR 25.0 with ber  0.0 with bler 0.0
no pos BER specified.
Test SNR 30.0 with ber  0.4983900487422943 with bler 1.0
Punctured Test SNR 30.0 with ber  0.0 with bler 0.0
no pos BER specified.
Test SNR 35.0 with ber  0.49908992648124695 with bler 1.0
Punctured Test SNR 35.0 with ber  0.0 with bler 0.0
no pos BER specified.
Test SNR 40.0 with ber  0.49945008754730225 with bler 1.0
Punctured Test SNR 40.0 with ber  0.0 with bler 0.0
no pos BER specified.
Test SNR 45.0 with ber  0.500809907913208 with bler 1.0
Punctured Test SNR 45.0 with ber  0.0 with bler 0.0
no pos BER specified.
Test SNR 50.0 with ber  0.5000399351119995 with bler 1.0
Punctured Test SNR 50.0 with ber  0.0 with bler 0.0
no pos BER specified.
Test SNR 55.0 with ber  0.49946004152297974 with bler 1.0
Punctured Test SNR 55.0 with ber  0.0 with bler 0.0
no pos BER specified.
Test SNR 60.0 with ber  0.5003799796104431 with bler 1.0
Punctured Test SNR 60.0 with ber  0.0 with bler 0.0
no pos BER specified.
Test SNR 65.0 with ber  0.5001499652862549 with bler 1.0
Punctured Test SNR 65.0 with ber  0.0 with bler 0.0
no pos BER specified.
Test SNR 70.0 with ber  0.5003599524497986 with bler 1.0
Punctured Test SNR 70.0 with ber  0.0 with bler 0.0
no pos BER specified.
Test SNR 75.0 with ber  0.4998500347137451 with bler 1.0
Punctured Test SNR 75.0 with ber  0.0 with bler 0.0
no pos BER specified.
Test SNR 80.0 with ber  0.4982199966907501 with bler 1.0
Punctured Test SNR 80.0 with ber  0.0 with bler 0.0
no pos BER specified.
Test SNR 85.0 with ber  0.5008600354194641 with bler 1.0
Punctured Test SNR 85.0 with ber  0.0 with bler 0.0
no pos BER specified.
Test SNR 90.0 with ber  0.5004000663757324 with bler 1.0
Punctured Test SNR 90.0 with ber  0.0 with bler 0.0
no pos BER specified.
Test SNR 95.0 with ber  0.49952006340026855 with bler 1.0
Punctured Test SNR 95.0 with ber  0.0 with bler 0.0
final results on SNRs  [0.0, 5.0, 10.0, 15.0, 20.0, 25.0, 30.0, 35.0, 40.0, 45.0, 50.0, 55.0, 60.0, 65.0, 70.0, 75.0, 80.0, 85.0, 90.0, 95.0]
BER [0.49999985098838806, 0.49772998690605164, 0.4984799921512604, 0.5013200640678406, 0.5015000700950623, 0.4983200430870056, 0.4983900487422943, 0.49908992648124695, 0.49945008754730225, 0.500809907913208, 0.5000399351119995, 0.49946004152297974, 0.5003799796104431, 0.5001499652862549, 0.5003599524497986, 0.4998500347137451, 0.4982199966907501, 0.5008600354194641, 0.5004000663757324, 0.49952006340026855]
BLER [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]
final results on punctured SNRs  [0.0, 5.0, 10.0, 15.0, 20.0, 25.0, 30.0, 35.0, 40.0, 45.0, 50.0, 55.0, 60.0, 65.0, 70.0, 75.0, 80.0, 85.0, 90.0, 95.0]
BER [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
BLER [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
encoder power is tensor(1.)
adjusted SNR should be [-0.0, 4.999999888090176, 10.00000005838476, 15.000000078774018, 19.999999870570157, 25.000000003171387, 30.000000467677864, 34.99999989681464, 40.00000019414476, 45.00000000317138, 50.000000083965574, 54.99999989681464, 59.99999958744239, 65.00000036280017, 70.00000024384569, 75.0000000389704, 80.00000021942404, 84.9999998008802, 89.99999964429526, 95.0000003943598]
