Namespace(channel='awgn', vv=5, radar_prob=0.05, radar_power=5.0, train_enc_channel_low=0.0, train_enc_channel_high=1.0, train_dec_channel_low=-1.5, train_dec_channel_high=2.0, init_nw_weight='default', code_rate_k=1, code_rate_n=2, enc_rnn='gru', dec_rnn='gru', enc_num_layer=3, dec_num_layer=3, dec_num_unit=100, enc_num_unit=25, enc_act='elu', dec_act='linear', num_train_dec=5, num_train_enc=1, dropout=0.0, snr_test_start=-1.5, snr_test_end=4.0, snr_points=12, batch_size=100, num_epoch=5, test_ratio=1, block_len=100, block_len_low=10, block_len_high=200, is_variable_block_len=False, num_block=1000, test_channel_mode='block_norm', train_channel_mode='block_norm', enc_truncate_limit=0, no_code_norm=False, enc_quantize_level=2, enc_value_limit=1.0, enc_grad_limit=0.01, enc_clipping='both', optimizer='adam', dec_lr=0.01, enc_lr=0.01, no_cuda=False, rec_quantize=False, print_pos_ber=False, print_pos_power=False, print_test_traj=False, precompute_norm_stats=False, D=1, BASE_PATH='C:\\WorkSpace\\FadingChannels\\Swetha_M20AIE317_MTP\\Fading', LOG_PATH='C:\\WorkSpace\\FadingChannels\\Swetha_M20AIE317_MTP\\Fading\\logs_faded', DATA_PATH='C:\\WorkSpace\\FadingChannels\\Swetha_M20AIE317_MTP\\Fading\\data_faded', MODEL_PATH='C:\\WorkSpace\\FadingChannels\\Swetha_M20AIE317_MTP\\Fading\\model_faded', PLOT_PATH='C:\\WorkSpace\\FadingChannels\\Swetha_M20AIE317_MTP\\Fading\\plots_faded')
use_cuda:  False
Channel_AE(
  (enc): ENC(
    (enc_rnn): GRU(1, 25, num_layers=3, batch_first=True)
    (enc_linear): Linear(in_features=25, out_features=2, bias=True)
  )
  (dec): DEC(
    (dropout): Dropout(p=0.0, inplace=False)
    (attention): Attention(
      (attn): Linear(in_features=200, out_features=1, bias=False)
    )
    (fc): Linear(in_features=200, out_features=100, bias=True)
    (dec1_rnns): GRU(2, 100, num_layers=3, batch_first=True)
    (dec2_rnns): GRU(2, 100, num_layers=3, batch_first=True)
    (dec_outputs): Linear(in_features=200, out_features=1, bias=True)
  )
)
====> Epoch: 1 Average loss: 0.69397376  running time 100.42823934555054
====> Epoch: 1 Average loss: 0.63616339  running time 99.94835019111633
====> Epoch: 1 Average loss: 0.40248103  running time 100.91406416893005
====> Epoch: 1 Average loss: 0.34585444  running time 100.17908334732056
====> Epoch: 1 Average loss: 0.32919688  running time 100.3822181224823
====> Epoch: 1 Average loss: 0.31843792  running time 101.7776882648468
====> Test set BCE loss 0.3158605396747589 Custom Loss 0.3158605396747589 with ber  0.13089999556541443 with bler  1.0
saved model C:\WorkSpace\FadingChannels\Swetha_M20AIE317_MTP\Fading\model_faded\attention_model_1_awgn_lr_0.01_D1_1000_20230315-214014.pt
each epoch training time: 610.2144391536713s
====> Epoch: 2 Average loss: 0.26866672  running time 99.97782397270203
====> Epoch: 2 Average loss: 0.22471221  running time 100.2560043334961
====> Epoch: 2 Average loss: 0.20753144  running time 99.92102980613708
====> Epoch: 2 Average loss: 0.20025387  running time 100.02801609039307
====> Epoch: 2 Average loss: 0.19439595  running time 100.60888671875
====> Epoch: 2 Average loss: 0.19560635  running time 101.52289962768555
====> Test set BCE loss 0.1976475566625595 Custom Loss 0.1976475566625595 with ber  0.07217000424861908 with bler  1.0
saved model C:\WorkSpace\FadingChannels\Swetha_M20AIE317_MTP\Fading\model_faded\attention_model_2_awgn_lr_0.01_D1_1000_20230315-214014.pt
each epoch training time: 609.045045375824s
====> Epoch: 3 Average loss: 0.19879392  running time 101.32631778717041
====> Epoch: 3 Average loss: 0.19001965  running time 99.73965501785278
====> Epoch: 3 Average loss: 0.18820219  running time 99.93174886703491
====> Epoch: 3 Average loss: 0.18567476  running time 101.35320615768433
====> Epoch: 3 Average loss: 0.18705636  running time 100.90713667869568
====> Epoch: 3 Average loss: 0.18836112  running time 100.54775428771973
====> Test set BCE loss 0.18476533889770508 Custom Loss 0.18476533889770508 with ber  0.06698000431060791 with bler  0.998
saved model C:\WorkSpace\FadingChannels\Swetha_M20AIE317_MTP\Fading\model_faded\attention_model_3_awgn_lr_0.01_D1_1000_20230315-214014.pt
each epoch training time: 610.5891678333282s
====> Epoch: 4 Average loss: 0.18805068  running time 100.15033411979675
====> Epoch: 4 Average loss: 0.18814671  running time 99.80775046348572
====> Epoch: 4 Average loss: 0.18663057  running time 100.01932287216187
====> Epoch: 4 Average loss: 0.18680195  running time 100.42554044723511
====> Epoch: 4 Average loss: 0.18551742  running time 100.79426097869873
====> Epoch: 4 Average loss: 0.18457307  running time 100.49891328811646
====> Test set BCE loss 0.1880928874015808 Custom Loss 0.1880928874015808 with ber  0.06843999773263931 with bler  0.9970000000000001
saved model C:\WorkSpace\FadingChannels\Swetha_M20AIE317_MTP\Fading\model_faded\attention_model_4_awgn_lr_0.01_D1_1000_20230315-214014.pt
each epoch training time: 608.4060537815094s
====> Epoch: 5 Average loss: 0.18647286  running time 99.9732620716095
====> Epoch: 5 Average loss: 0.18581739  running time 99.83632516860962
====> Epoch: 5 Average loss: 0.18408248  running time 100.87161159515381
====> Epoch: 5 Average loss: 0.18459300  running time 100.25905561447144
====> Epoch: 5 Average loss: 0.18615499  running time 100.34786796569824
====> Epoch: 5 Average loss: 0.18326657  running time 99.75423884391785
====> Test set BCE loss 0.18739967048168182 Custom Loss 0.18739967048168182 with ber  0.06763999909162521 with bler  1.0
saved model C:\WorkSpace\FadingChannels\Swetha_M20AIE317_MTP\Fading\model_faded\attention_model_5_awgn_lr_0.01_D1_1000_20230315-214014.pt
each epoch training time: 607.7659223079681s
saved model C:\WorkSpace\FadingChannels\Swetha_M20AIE317_MTP\Fading\model_faded\attention_model_awgn_lr_0.01_D1_1000.pt
SNRS [-2.   0.2  2.4  4.6  6.8  9.  11.2 13.4 15.6 17.8 20. ]
Test SNR -2.0 with ber  0.085999995470047 with awgn ber  0.7473799999999999 with rayleigh ber  0.95566 with bler 1.0
Test SNR 0.20000000000000018 with ber  0.06404999643564224 with awgn ber  0.54833 with rayleigh ber  0.7769799999999999 with bler 0.9949999999999999
Test SNR 2.4000000000000004 with ber  0.04659000039100647 with awgn ber  0.35924500000000004 with rayleigh ber  0.59772 with bler 0.9870000000000001
Test SNR 4.6000000000000005 with ber  0.03340999782085419 with awgn ber  0.193855 with rayleigh ber  0.4386150000000001 with bler 0.966
Test SNR 6.800000000000001 with ber  0.022590000182390213 with awgn ber  0.07522 with rayleigh ber  0.30673000000000006 with bler 0.9049999999999999
Test SNR 9.0 with ber  0.014369999058544636 with awgn ber  0.01793 with rayleigh ber  0.20646499999999998 with bler 0.7569999999999999
Test SNR 11.200000000000001 with ber  0.009940000250935555 with awgn ber  0.0017100000000000001 with rayleigh ber  0.133435 with bler 0.6410000000000001
Test SNR 13.400000000000002 with ber  0.006319999694824219 with awgn ber  4e-05 with rayleigh ber  0.08390499999999998 with bler 0.4760000000000001
Test SNR 15.600000000000001 with ber  0.004430000204592943 with awgn ber  0.0 with rayleigh ber  0.051985 with bler 0.36100000000000004
Test SNR 17.8 with ber  0.002630000002682209 with awgn ber  0.0 with rayleigh ber  0.032580000000000005 with bler 0.23800000000000004
Test SNR 20.0 with ber  0.0016999999061226845 with awgn ber  0.0 with rayleigh ber  0.018940000000000002 with bler 0.14900000000000002
final results on SNRs  [-2.   0.2  2.4  4.6  6.8  9.  11.2 13.4 15.6 17.8 20. ]
BER [0.085999995470047, 0.06404999643564224, 0.04659000039100647, 0.03340999782085419, 0.022590000182390213, 0.014369999058544636, 0.009940000250935555, 0.006319999694824219, 0.004430000204592943, 0.002630000002682209, 0.0016999999061226845]
BLER [1.0, 0.9949999999999999, 0.9870000000000001, 0.966, 0.9049999999999999, 0.7569999999999999, 0.6410000000000001, 0.4760000000000001, 0.36100000000000004, 0.23800000000000004, 0.14900000000000002]
encoder power is 1.0
adjusted SNR should be [-2.000000180303801, 0.19999996779759982, 2.399999883308927, 4.60000017244041, 6.799999760125526, 8.99999981157327, 11.199999975432803, 13.399999824531744, 15.600000161235467, 17.80000039812418, 19.999999870570157]
