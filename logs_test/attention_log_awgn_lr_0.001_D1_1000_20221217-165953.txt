Namespace(channel='awgn', vv=5, radar_prob=0.05, radar_power=5.0, train_enc_channel_low=1.0, train_enc_channel_high=1.0, train_dec_channel_low=-1.5, train_dec_channel_high=2.0, init_nw_weight='default', code_rate_k=1, code_rate_n=3, enc_rnn='gru', dec_rnn='gru', enc_num_layer=2, dec_num_layer=2, dec_num_unit=100, enc_num_unit=25, enc_act='elu', dec_act='linear', num_train_dec=5, num_train_enc=1, dropout=0.0, snr_test_start=-1.5, snr_test_end=4.0, snr_points=12, batch_size=100, num_epoch=1, test_ratio=1, block_len=100, block_len_low=10, block_len_high=200, is_variable_block_len=False, num_block=1000, test_channel_mode='block_norm', train_channel_mode='block_norm', enc_truncate_limit=0, no_code_norm=False, enc_quantize_level=2, enc_value_limit=1.0, enc_grad_limit=0.01, enc_clipping='both', optimizer='adam', dec_lr=0.001, enc_lr=0.001, no_cuda=False, rec_quantize=False, print_pos_ber=False, print_pos_power=False, print_test_traj=False, precompute_norm_stats=False, D=1)
use_cuda:  False
Channel_AE(
  (enc): ENC(
    (enc_rnn): GRU(1, 25, num_layers=2, batch_first=True)
    (enc_linear): Linear(in_features=25, out_features=3, bias=True)
  )
  (dec): DEC(
    (dropout): Dropout(p=0.0, inplace=False)
    (attention): Attention(
      (attn): Linear(in_features=200, out_features=1, bias=False)
    )
    (fc): Linear(in_features=200, out_features=100, bias=True)
    (dec1_rnns): GRU(3, 100, num_layers=2, batch_first=True)
    (dec2_rnns): GRU(3, 100, num_layers=2, batch_first=True)
    (dec_outputs): Linear(in_features=200, out_features=1, bias=True)
  )
)
====> Epoch: 1 Average loss: 0.69179997  running time 148.49826383590698
====> Epoch: 1 Average loss: 0.66832687  running time 137.2379698753357
====> Epoch: 1 Average loss: 0.58579927  running time 131.31327843666077
====> Epoch: 1 Average loss: 0.50396902  running time 141.26269125938416
====> Epoch: 1 Average loss: 0.47632670  running time 140.27572751045227
====> Epoch: 1 Average loss: 0.47196064  running time 140.45704007148743
====> Test set BCE loss 0.44587022066116333 Custom Loss 0.44587022066116333 with ber  0.20945999026298523 with bler  1.0
saved model C:\WorkSpace\Swetha_M20AIE317_MTP\model_test\attention_model_1_awgn_lr_0.001_D1_1000_20221217-165953.pt
each epoch training time: 847.7721121311188s
saved model C:\WorkSpace\Swetha_M20AIE317_MTP\model_test\attention_model_awgn_lr_0.001_D1_1000.pt
SNRS [-1.5, -1.0, -0.5, 0.0, 0.5, 1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0]
Test SNR -1.5 with ber  0.2522200047969818 with bler 1.0
Test SNR -1.0 with ber  0.24249999225139618 with bler 1.0
Test SNR -0.5 with ber  0.23645000159740448 with bler 1.0
Test SNR 0.0 with ber  0.22688999772071838 with bler 1.0
Test SNR 0.5 with ber  0.21585997939109802 with bler 1.0
Test SNR 1.0 with ber  0.2085999995470047 with bler 1.0
Test SNR 1.5 with ber  0.2008800059556961 with bler 1.0
Test SNR 2.0 with ber  0.19181999564170837 with bler 1.0
Test SNR 2.5 with ber  0.18288999795913696 with bler 1.0
Test SNR 3.0 with ber  0.17664000391960144 with bler 1.0
Test SNR 3.5 with ber  0.16773000359535217 with bler 1.0
Test SNR 4.0 with ber  0.1620599925518036 with bler 1.0
final results on SNRs  [-1.5, -1.0, -0.5, 0.0, 0.5, 1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0]
BER [0.2522200047969818, 0.24249999225139618, 0.23645000159740448, 0.22688999772071838, 0.21585997939109802, 0.2085999995470047, 0.2008800059556961, 0.19181999564170837, 0.18288999795913696, 0.17664000391960144, 0.16773000359535217, 0.1620599925518036]
BLER [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]
encoder power is 1.0
adjusted SNR should be [-1.4999997446509226, -1.0000000166986343, -0.49999973308696327, -0.0, 0.5000001308463472, 1.0000002900227403, 1.5000000201403676, 2.0000002404171053, 2.5000000877622415, 3.0000002493010487, 3.500000207085638, 3.999999717024358]
