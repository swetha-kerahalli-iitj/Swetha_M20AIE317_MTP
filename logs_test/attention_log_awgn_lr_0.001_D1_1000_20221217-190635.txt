Namespace(channel='awgn', vv=5, radar_prob=0.05, radar_power=5.0, train_enc_channel_low=1.0, train_enc_channel_high=1.0, train_dec_channel_low=-1.5, train_dec_channel_high=2.0, init_nw_weight='default', code_rate_k=1, code_rate_n=3, enc_rnn='gru', dec_rnn='gru', enc_num_layer=2, dec_num_layer=2, dec_num_unit=100, enc_num_unit=25, enc_act='elu', dec_act='linear', num_train_dec=5, num_train_enc=1, dropout=0.0, snr_test_start=-1.5, snr_test_end=4.0, snr_points=12, batch_size=100, num_epoch=1, test_ratio=1, block_len=100, block_len_low=10, block_len_high=200, is_variable_block_len=False, num_block=1000, test_channel_mode='block_norm', train_channel_mode='block_norm', enc_truncate_limit=0, no_code_norm=False, enc_quantize_level=2, enc_value_limit=1.0, enc_grad_limit=0.01, enc_clipping='both', optimizer='adam', dec_lr=0.001, enc_lr=0.001, no_cuda=False, rec_quantize=False, print_pos_ber=False, print_pos_power=False, print_test_traj=False, precompute_norm_stats=False, D=1)
use_cuda:  False
Channel_AE(
  (enc): ENC(
    (enc_rnn): GRU(1, 25, num_layers=2, batch_first=True)
    (enc_linear): Linear(in_features=25, out_features=3, bias=True)
  )
  (dec): DEC(
    (dropout): Dropout(p=0.0, inplace=False)
    (attention): Attention(
      (attn): Linear(in_features=200, out_features=1, bias=False)
    )
    (fc): Linear(in_features=200, out_features=100, bias=True)
    (dec1_rnns): GRU(3, 100, num_layers=2, batch_first=True)
    (dec2_rnns): GRU(3, 100, num_layers=2, batch_first=True)
    (dec_outputs): Linear(in_features=200, out_features=1, bias=True)
  )
)
====> Epoch: 1 Average loss: 0.69400018  running time 117.54138731956482
====> Epoch: 1 Average loss: 0.69301518  running time 115.35978722572327
====> Epoch: 1 Average loss: 0.69121878  running time 114.54147148132324
====> Epoch: 1 Average loss: 0.68990047  running time 126.26879549026489
====> Epoch: 1 Average loss: 0.68993940  running time 126.66407752037048
====> Epoch: 1 Average loss: 0.68956339  running time 124.84309005737305
====> Test set BCE loss 0.6887589693069458 Custom Loss 0.6887589693069458 with ber  0.46219998598098755 with bler  1.0
saved model C:\WorkSpace\Swetha_M20AIE317_MTP\model_test\attention_model_1_awgn_lr_0.001_D1_1000_20221217-190635.pt
each epoch training time: 731.4057366847992s
saved model C:\WorkSpace\Swetha_M20AIE317_MTP\model_test\attention_model_awgn_lr_0.001_D1_1000.pt
SNRS [-1.5, -1.0, -0.5, 0.0, 0.5, 1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0]
Test SNR -1.5 with ber  0.4722300171852112 with bler 1.0
Test SNR -1.0 with ber  0.47053998708724976 with bler 1.0
Test SNR -0.5 with ber  0.4677700102329254 with bler 1.0
Test SNR 0.0 with ber  0.4654099941253662 with bler 1.0
Test SNR 0.5 with ber  0.46511000394821167 with bler 1.0
Test SNR 1.0 with ber  0.46142998337745667 with bler 1.0
Test SNR 1.5 with ber  0.45844000577926636 with bler 1.0
Test SNR 2.0 with ber  0.45486998558044434 with bler 1.0
Test SNR 2.5 with ber  0.45454996824264526 with bler 1.0
Test SNR 3.0 with ber  0.45402997732162476 with bler 1.0
Test SNR 3.5 with ber  0.44944995641708374 with bler 1.0
Test SNR 4.0 with ber  0.44600996375083923 with bler 1.0
final results on SNRs  [-1.5, -1.0, -0.5, 0.0, 0.5, 1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0]
BER [0.4722300171852112, 0.47053998708724976, 0.4677700102329254, 0.4654099941253662, 0.46511000394821167, 0.46142998337745667, 0.45844000577926636, 0.45486998558044434, 0.45454996824264526, 0.45402997732162476, 0.44944995641708374, 0.44600996375083923]
BLER [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]
encoder power is 1.0
adjusted SNR should be [-1.4999997446509226, -1.0000000166986343, -0.49999973308696327, -0.0, 0.5000001308463472, 1.0000002900227403, 1.5000000201403676, 2.0000002404171053, 2.5000000877622415, 3.0000002493010487, 3.500000207085638, 3.999999717024358]
Training Time: 827.7061440944672s
