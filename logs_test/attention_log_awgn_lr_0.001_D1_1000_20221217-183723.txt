Namespace(channel='awgn', vv=5, radar_prob=0.05, radar_power=5.0, train_enc_channel_low=1.0, train_enc_channel_high=1.0, train_dec_channel_low=-1.5, train_dec_channel_high=2.0, init_nw_weight='default', code_rate_k=1, code_rate_n=3, enc_rnn='gru', dec_rnn='gru', enc_num_layer=2, dec_num_layer=2, dec_num_unit=100, enc_num_unit=25, enc_act='elu', dec_act='linear', num_train_dec=5, num_train_enc=1, dropout=0.0, snr_test_start=-1.5, snr_test_end=4.0, snr_points=12, batch_size=100, num_epoch=1, test_ratio=1, block_len=100, block_len_low=10, block_len_high=200, is_variable_block_len=False, num_block=1000, test_channel_mode='block_norm', train_channel_mode='block_norm', enc_truncate_limit=0, no_code_norm=False, enc_quantize_level=2, enc_value_limit=1.0, enc_grad_limit=0.01, enc_clipping='both', optimizer='adam', dec_lr=0.001, enc_lr=0.001, no_cuda=False, rec_quantize=False, print_pos_ber=False, print_pos_power=False, print_test_traj=False, precompute_norm_stats=False, D=1)
use_cuda:  False
Channel_AE(
  (enc): ENC(
    (enc_rnn): GRU(1, 25, num_layers=2, batch_first=True)
    (enc_linear): Linear(in_features=25, out_features=3, bias=True)
  )
  (dec): DEC(
    (dropout): Dropout(p=0.0, inplace=False)
    (attention): Attention(
      (attn): Linear(in_features=200, out_features=1, bias=False)
    )
    (fc): Linear(in_features=200, out_features=100, bias=True)
    (dec1_rnns): GRU(3, 100, num_layers=2, batch_first=True)
    (dec2_rnns): GRU(3, 100, num_layers=2, batch_first=True)
    (dec_outputs): Linear(in_features=200, out_features=1, bias=True)
  )
)
====> Epoch: 1 Average loss: 0.69335725  running time 117.13208270072937
====> Epoch: 1 Average loss: 0.66937169  running time 138.39514827728271
====> Epoch: 1 Average loss: 0.58528646  running time 117.09788346290588
====> Epoch: 1 Average loss: 0.47912018  running time 111.38747835159302
====> Epoch: 1 Average loss: 0.45587930  running time 109.76811242103577
====> Epoch: 1 Average loss: 0.45121277  running time 110.7674491405487
====> Test set BCE loss 0.41835254430770874 Custom Loss 0.41835254430770874 with ber  0.19298000633716583 with bler  1.0
saved model C:\WorkSpace\Swetha_M20AIE317_MTP\model_test\attention_model_1_awgn_lr_0.001_D1_1000_20221217-183723.pt
each epoch training time: 709.6213343143463s
saved model C:\WorkSpace\Swetha_M20AIE317_MTP\model_test\attention_model_awgn_lr_0.001_D1_1000.pt
SNRS [-1.5, -1.0, -0.5, 0.0, 0.5, 1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0]
Test SNR -1.5 with ber  0.24095001816749573 with bler 1.0
Test SNR -1.0 with ber  0.23129001259803772 with bler 1.0
Test SNR -0.5 with ber  0.22123000025749207 with bler 1.0
Test SNR 0.0 with ber  0.2102999985218048 with bler 1.0
Test SNR 0.5 with ber  0.2002599686384201 with bler 1.0
Test SNR 1.0 with ber  0.19217000901699066 with bler 1.0
Test SNR 1.5 with ber  0.18091998994350433 with bler 1.0
Test SNR 2.0 with ber  0.17254000902175903 with bler 1.0
Test SNR 2.5 with ber  0.16275998950004578 with bler 1.0
Test SNR 3.0 with ber  0.1514499932527542 with bler 1.0
Test SNR 3.5 with ber  0.1450600028038025 with bler 1.0
Test SNR 4.0 with ber  0.13832999765872955 with bler 1.0
final results on SNRs  [-1.5, -1.0, -0.5, 0.0, 0.5, 1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0]
BER [0.24095001816749573, 0.23129001259803772, 0.22123000025749207, 0.2102999985218048, 0.2002599686384201, 0.19217000901699066, 0.18091998994350433, 0.17254000902175903, 0.16275998950004578, 0.1514499932527542, 0.1450600028038025, 0.13832999765872955]
BLER [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]
encoder power is 1.0
adjusted SNR should be [-1.4999997446509226, -1.0000000166986343, -0.49999973308696327, -0.0, 0.5000001308463472, 1.0000002900227403, 1.5000000201403676, 2.0000002404171053, 2.5000000877622415, 3.0000002493010487, 3.500000207085638, 3.999999717024358]
