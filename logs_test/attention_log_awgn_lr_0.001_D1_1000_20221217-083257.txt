Namespace(channel='awgn', vv=5, radar_prob=0.05, radar_power=5.0, train_enc_channel_low=1.0, train_enc_channel_high=1.0, train_dec_channel_low=-1.5, train_dec_channel_high=2.0, init_nw_weight='default', code_rate_k=1, code_rate_n=3, enc_rnn='gru', dec_rnn='gru', enc_num_layer=2, dec_num_layer=2, dec_num_unit=100, enc_num_unit=25, enc_act='elu', dec_act='linear', num_train_dec=5, num_train_enc=1, dropout=0.0, snr_test_start=-1.5, snr_test_end=4.0, snr_points=12, batch_size=100, num_epoch=1, test_ratio=1, block_len=100, block_len_low=10, block_len_high=200, is_variable_block_len=False, num_block=1000, test_channel_mode='block_norm', train_channel_mode='block_norm', enc_truncate_limit=0, no_code_norm=False, enc_quantize_level=2, enc_value_limit=1.0, enc_grad_limit=0.01, enc_clipping='both', optimizer='adam', dec_lr=0.001, enc_lr=0.001, no_cuda=False, rec_quantize=False, print_pos_ber=False, print_pos_power=False, print_test_traj=False, precompute_norm_stats=False, D=1)
use_cuda:  False
Channel_AE(
  (enc): ENC(
    (enc_rnn): GRU(1, 25, num_layers=2, batch_first=True)
    (enc_linear): Linear(in_features=25, out_features=3, bias=True)
  )
  (dec): DEC(
    (dropout): Dropout(p=0.0, inplace=False)
    (attention): Attention(
      (attn): Linear(in_features=200, out_features=1, bias=False)
    )
    (fc): Linear(in_features=200, out_features=100, bias=True)
    (dec1_rnns): GRU(3, 100, num_layers=2, batch_first=True)
    (dec2_rnns): GRU(3, 100, num_layers=2, batch_first=True)
    (dec_outputs): Linear(in_features=200, out_features=1, bias=True)
  )
)
====> Epoch: 1 Average loss: 0.69314416  running time 87.84206676483154
====> Epoch: 1 Average loss: 0.69341804  running time 87.64424180984497
====> Epoch: 1 Average loss: 0.69127141  running time 87.41661930084229
====> Epoch: 1 Average loss: 0.68932511  running time 87.30730938911438
====> Epoch: 1 Average loss: 0.68796415  running time 87.44630479812622
====> Epoch: 1 Average loss: 0.68652513  running time 87.3651750087738
====> Test set BCE loss 0.6862804293632507 Custom Loss 0.6862804293632507 with ber  0.4536799490451813 with bler  1.0
saved model C:\WorkSpace\Swetha_M20AIE317_MTP\model_test\attention_model_1_awgn_lr_0.001_D1_1000_20221217-083257.pt
each epoch training time: 529.1544103622437s
saved model C:\WorkSpace\Swetha_M20AIE317_MTP\model_test\attention_model_awgn_lr_0.001_D1_1000.pt
SNRS [-1.5, -1.0, -0.5, 0.0, 0.5, 1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0]
Test SNR -1.5 with ber  0.4665699899196625 with bler 1.0
Test SNR -1.0 with ber  0.4612899720668793 with bler 1.0
Test SNR -0.5 with ber  0.4597899913787842 with bler 1.0
Test SNR 0.0 with ber  0.455670028924942 with bler 1.0
Test SNR 0.5 with ber  0.4527699947357178 with bler 1.0
Test SNR 1.0 with ber  0.4550899565219879 with bler 1.0
Test SNR 1.5 with ber  0.4490899443626404 with bler 1.0
Test SNR 2.0 with ber  0.44495001435279846 with bler 1.0
Test SNR 2.5 with ber  0.4408499598503113 with bler 1.0
Test SNR 3.0 with ber  0.4423900246620178 with bler 1.0
Test SNR 3.5 with ber  0.4376800060272217 with bler 1.0
Test SNR 4.0 with ber  0.4333200454711914 with bler 1.0
final results on SNRs  [-1.5, -1.0, -0.5, 0.0, 0.5, 1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0]
BER [0.4665699899196625, 0.4612899720668793, 0.4597899913787842, 0.455670028924942, 0.4527699947357178, 0.4550899565219879, 0.4490899443626404, 0.44495001435279846, 0.4408499598503113, 0.4423900246620178, 0.4376800060272217, 0.4333200454711914]
BLER [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]
encoder power is 1.0
adjusted SNR should be [-1.4999997446509226, -1.0000000166986343, -0.49999973308696327, -0.0, 0.5000001308463472, 1.0000002900227403, 1.5000000201403676, 2.0000002404171053, 2.5000000877622415, 3.0000002493010487, 3.500000207085638, 3.999999717024358]
