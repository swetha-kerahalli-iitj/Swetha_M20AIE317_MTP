Namespace(channel='awgn', vv=5, radar_prob=0.05, radar_power=5.0, train_enc_channel_low=1.0, train_enc_channel_high=1.0, train_dec_channel_low=-1.5, train_dec_channel_high=2.0, init_nw_weight='default', code_rate_k=1, code_rate_n=3, enc_rnn='gru', dec_rnn='gru', enc_num_layer=2, dec_num_layer=2, dec_num_unit=100, enc_num_unit=25, enc_act='elu', dec_act='linear', num_train_dec=5, num_train_enc=1, dropout=0.0, snr_test_start=-1.5, snr_test_end=4.0, snr_points=12, batch_size=100, num_epoch=1, test_ratio=1, block_len=100, block_len_low=10, block_len_high=200, is_variable_block_len=False, num_block=1000, test_channel_mode='block_norm', train_channel_mode='block_norm', enc_truncate_limit=0, no_code_norm=False, enc_quantize_level=2, enc_value_limit=1.0, enc_grad_limit=0.01, enc_clipping='both', optimizer='adam', dec_lr=0.001, enc_lr=0.001, no_cuda=False, rec_quantize=False, print_pos_ber=False, print_pos_power=False, print_test_traj=False, precompute_norm_stats=False, D=1)
use_cuda:  False
Channel_AE(
  (enc): ENC(
    (enc_rnn): GRU(1, 25, num_layers=2, batch_first=True)
    (enc_linear): Linear(in_features=25, out_features=3, bias=True)
  )
  (dec): DEC(
    (dropout): Dropout(p=0.0, inplace=False)
    (attention): Attention(
      (attn): Linear(in_features=200, out_features=1, bias=False)
    )
    (fc): Linear(in_features=200, out_features=100, bias=True)
    (dec1_rnns): GRU(3, 100, num_layers=2, batch_first=True)
    (dec2_rnns): GRU(3, 100, num_layers=2, batch_first=True)
    (dec_outputs): Linear(in_features=200, out_features=1, bias=True)
  )
)
====> Epoch: 1 Average loss: 0.69322697  running time 89.66484522819519
====> Epoch: 1 Average loss: 0.68936143  running time 89.06243848800659
====> Epoch: 1 Average loss: 0.67570883  running time 86.99677777290344
====> Epoch: 1 Average loss: 0.66265233  running time 87.038339138031
====> Epoch: 1 Average loss: 0.65751284  running time 88.13356852531433
====> Epoch: 1 Average loss: 0.65843320  running time 87.04332232475281
====> Test set BCE loss 0.6522887945175171 Custom Loss 0.6522887945175171 with ber  0.38264000415802 with bler  1.0
saved model C:\WorkSpace\Swetha_M20AIE317_MTP\model_test\attention_model_1_awgn_lr_0.001_D1_1000_20221217-073516.pt
each epoch training time: 532.050021648407s
saved model C:\WorkSpace\Swetha_M20AIE317_MTP\model_test\attention_model_awgn_lr_0.001_D1_1000.pt
SNRS [-1.5, -1.0, -0.5, 0.0, 0.5, 1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0]
Test SNR -1.5 with ber  0.4049500524997711 with bler 1.0
Test SNR -1.0 with ber  0.4018000066280365 with bler 1.0
Test SNR -0.5 with ber  0.3958600163459778 with bler 1.0
Test SNR 0.0 with ber  0.3923000395298004 with bler 1.0
Test SNR 0.5 with ber  0.3864700198173523 with bler 1.0
Test SNR 1.0 with ber  0.38380998373031616 with bler 1.0
Test SNR 1.5 with ber  0.375029981136322 with bler 1.0
Test SNR 2.0 with ber  0.36906999349594116 with bler 1.0
Test SNR 2.5 with ber  0.3654100000858307 with bler 1.0
Test SNR 3.0 with ber  0.35982000827789307 with bler 1.0
Test SNR 3.5 with ber  0.3511900007724762 with bler 1.0
Test SNR 4.0 with ber  0.34529998898506165 with bler 1.0
final results on SNRs  [-1.5, -1.0, -0.5, 0.0, 0.5, 1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0]
BER [0.4049500524997711, 0.4018000066280365, 0.3958600163459778, 0.3923000395298004, 0.3864700198173523, 0.38380998373031616, 0.375029981136322, 0.36906999349594116, 0.3654100000858307, 0.35982000827789307, 0.3511900007724762, 0.34529998898506165]
BLER [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]
encoder power is 1.0
adjusted SNR should be [-1.4999997446509226, -1.0000000166986343, -0.49999973308696327, -0.0, 0.5000001308463472, 1.0000002900227403, 1.5000000201403676, 2.0000002404171053, 2.5000000877622415, 3.0000002493010487, 3.500000207085638, 3.999999717024358]
