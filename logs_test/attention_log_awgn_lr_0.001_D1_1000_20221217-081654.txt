Namespace(channel='awgn', vv=5, radar_prob=0.05, radar_power=5.0, train_enc_channel_low=1.0, train_enc_channel_high=1.0, train_dec_channel_low=-1.5, train_dec_channel_high=2.0, init_nw_weight='default', code_rate_k=1, code_rate_n=3, enc_rnn='gru', dec_rnn='gru', enc_num_layer=2, dec_num_layer=2, dec_num_unit=100, enc_num_unit=25, enc_act='elu', dec_act='linear', num_train_dec=5, num_train_enc=1, dropout=0.0, snr_test_start=-1.5, snr_test_end=4.0, snr_points=12, batch_size=100, num_epoch=1, test_ratio=1, block_len=100, block_len_low=10, block_len_high=200, is_variable_block_len=False, num_block=1000, test_channel_mode='block_norm', train_channel_mode='block_norm', enc_truncate_limit=0, no_code_norm=False, enc_quantize_level=2, enc_value_limit=1.0, enc_grad_limit=0.01, enc_clipping='both', optimizer='adam', dec_lr=0.001, enc_lr=0.001, no_cuda=False, rec_quantize=False, print_pos_ber=False, print_pos_power=False, print_test_traj=False, precompute_norm_stats=False, D=1)
use_cuda:  False
Channel_AE(
  (enc): ENC(
    (enc_rnn): GRU(1, 25, num_layers=2, batch_first=True)
    (enc_linear): Linear(in_features=25, out_features=3, bias=True)
  )
  (dec): DEC(
    (dropout): Dropout(p=0.0, inplace=False)
    (attention): Attention(
      (attn): Linear(in_features=200, out_features=1, bias=False)
    )
    (fc): Linear(in_features=200, out_features=100, bias=True)
    (dec1_rnns): GRU(3, 100, num_layers=2, batch_first=True)
    (dec2_rnns): GRU(3, 100, num_layers=2, batch_first=True)
    (dec_outputs): Linear(in_features=200, out_features=1, bias=True)
  )
)
====> Epoch: 1 Average loss: 0.69307935  running time 87.48335480690002
====> Epoch: 1 Average loss: 0.68182806  running time 87.18186116218567
====> Epoch: 1 Average loss: 0.63738255  running time 87.20160007476807
====> Epoch: 1 Average loss: 0.59070858  running time 86.76581072807312
====> Epoch: 1 Average loss: 0.58393817  running time 86.81685900688171
====> Epoch: 1 Average loss: 0.57862186  running time 86.9001681804657
====> Test set BCE loss 0.5622011423110962 Custom Loss 0.5622011423110962 with ber  0.29149001836776733 with bler  1.0
saved model C:\WorkSpace\Swetha_M20AIE317_MTP\model_test\attention_model_1_awgn_lr_0.001_D1_1000_20221217-081654.pt
each epoch training time: 526.4638895988464s
saved model C:\WorkSpace\Swetha_M20AIE317_MTP\model_test\attention_model_awgn_lr_0.001_D1_1000.pt
SNRS [-1.5, -1.0, -0.5, 0.0, 0.5, 1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0]
Test SNR -1.5 with ber  0.3315199911594391 with bler 1.0
Test SNR -1.0 with ber  0.32545000314712524 with bler 1.0
Test SNR -0.5 with ber  0.3145100176334381 with bler 1.0
Test SNR 0.0 with ber  0.30803999304771423 with bler 1.0
Test SNR 0.5 with ber  0.30100998282432556 with bler 1.0
Test SNR 1.0 with ber  0.2894900143146515 with bler 1.0
Test SNR 1.5 with ber  0.28412002325057983 with bler 1.0
Test SNR 2.0 with ber  0.2757899761199951 with bler 1.0
Test SNR 2.5 with ber  0.26975998282432556 with bler 1.0
Test SNR 3.0 with ber  0.2572999894618988 with bler 1.0
Test SNR 3.5 with ber  0.25255000591278076 with bler 1.0
Test SNR 4.0 with ber  0.24846000969409943 with bler 1.0
final results on SNRs  [-1.5, -1.0, -0.5, 0.0, 0.5, 1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0]
BER [0.3315199911594391, 0.32545000314712524, 0.3145100176334381, 0.30803999304771423, 0.30100998282432556, 0.2894900143146515, 0.28412002325057983, 0.2757899761199951, 0.26975998282432556, 0.2572999894618988, 0.25255000591278076, 0.24846000969409943]
BLER [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]
encoder power is 1.0
adjusted SNR should be [-1.4999997446509226, -1.0000000166986343, -0.49999973308696327, -0.0, 0.5000001308463472, 1.0000002900227403, 1.5000000201403676, 2.0000002404171053, 2.5000000877622415, 3.0000002493010487, 3.500000207085638, 3.999999717024358]
