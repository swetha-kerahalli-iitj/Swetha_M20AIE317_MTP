Namespace(channel='awgn', vv=5, radar_prob=0.05, radar_power=5.0, train_enc_channel_low=1.0, train_enc_channel_high=1.0, train_dec_channel_low=-1.5, train_dec_channel_high=2.0, init_nw_weight='default', code_rate_k=1, code_rate_n=3, enc_rnn='gru', dec_rnn='gru', enc_num_layer=2, dec_num_layer=2, dec_num_unit=100, enc_num_unit=25, enc_act='elu', dec_act='linear', num_train_dec=5, num_train_enc=1, dropout=0.0, snr_test_start=-1.5, snr_test_end=4.0, snr_points=12, batch_size=100, num_epoch=1, test_ratio=1, block_len=100, block_len_low=10, block_len_high=200, is_variable_block_len=False, num_block=1000, test_channel_mode='block_norm', train_channel_mode='block_norm', enc_truncate_limit=0, no_code_norm=False, enc_quantize_level=2, enc_value_limit=1.0, enc_grad_limit=0.01, enc_clipping='both', optimizer='adam', dec_lr=0.001, enc_lr=0.001, no_cuda=False, rec_quantize=False, print_pos_ber=False, print_pos_power=False, print_test_traj=False, precompute_norm_stats=False, D=1)
use_cuda:  False
Channel_AE(
  (enc): ENC(
    (enc_rnn): GRU(1, 25, num_layers=2, batch_first=True)
    (enc_linear): Linear(in_features=25, out_features=3, bias=True)
  )
  (dec): DEC(
    (dropout): Dropout(p=0.0, inplace=False)
    (attention): Attention(
      (attn): Linear(in_features=200, out_features=1, bias=False)
    )
    (fc): Linear(in_features=200, out_features=100, bias=True)
    (dec1_rnns): GRU(3, 100, num_layers=2, batch_first=True)
    (dec2_rnns): GRU(3, 100, num_layers=2, batch_first=True)
    (dec_outputs): Linear(in_features=200, out_features=1, bias=True)
  )
)
====> Epoch: 1 Average loss: 0.69386507  running time 137.86134600639343
====> Epoch: 1 Average loss: 0.69321213  running time 140.46993899345398
====> Epoch: 1 Average loss: 0.69203997  running time 139.2874460220337
====> Epoch: 1 Average loss: 0.69164240  running time 139.0843050479889
====> Epoch: 1 Average loss: 0.69093195  running time 140.79474210739136
====> Epoch: 1 Average loss: 0.69116848  running time 139.25928783416748
====> Test set BCE loss 0.6905957460403442 Custom Loss 0.6905957460403442 with ber  0.47099995613098145 with bler  1.0
saved model C:\WorkSpace\Swetha_M20AIE317_MTP\model_test\attention_model_1_awgn_lr_0.001_D1_1000_20221217-171626.pt
each epoch training time: 845.7983827590942s
saved model C:\WorkSpace\Swetha_M20AIE317_MTP\model_test\attention_model_awgn_lr_0.001_D1_1000.pt
SNRS [-1.5, -1.0, -0.5, 0.0, 0.5, 1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0]
Test SNR -1.5 with ber  0.48072999715805054 with bler 1.0
Test SNR -1.0 with ber  0.4777500033378601 with bler 1.0
Test SNR -0.5 with ber  0.4768400192260742 with bler 1.0
Test SNR 0.0 with ber  0.47415000200271606 with bler 1.0
Test SNR 0.5 with ber  0.47145000100135803 with bler 1.0
Test SNR 1.0 with ber  0.4736800193786621 with bler 1.0
Test SNR 1.5 with ber  0.47186002135276794 with bler 1.0
Test SNR 2.0 with ber  0.46838003396987915 with bler 1.0
Test SNR 2.5 with ber  0.4680200219154358 with bler 1.0
Test SNR 3.0 with ber  0.4624199867248535 with bler 1.0
Test SNR 3.5 with ber  0.46492999792099 with bler 1.0
Test SNR 4.0 with ber  0.4638800621032715 with bler 1.0
final results on SNRs  [-1.5, -1.0, -0.5, 0.0, 0.5, 1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0]
BER [0.48072999715805054, 0.4777500033378601, 0.4768400192260742, 0.47415000200271606, 0.47145000100135803, 0.4736800193786621, 0.47186002135276794, 0.46838003396987915, 0.4680200219154358, 0.4624199867248535, 0.46492999792099, 0.4638800621032715]
BLER [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]
encoder power is 1.0
adjusted SNR should be [-1.4999997446509226, -1.0000000166986343, -0.49999973308696327, -0.0, 0.5000001308463472, 1.0000002900227403, 1.5000000201403676, 2.0000002404171053, 2.5000000877622415, 3.0000002493010487, 3.500000207085638, 3.999999717024358]
