Namespace(channel='awgn', vv=5, radar_prob=0.05, radar_power=5.0, train_enc_channel_low=1.0, train_enc_channel_high=1.0, train_dec_channel_low=-1.5, train_dec_channel_high=2.0, init_nw_weight='default', code_rate_k=1, code_rate_n=3, enc_rnn='gru', dec_rnn='gru', enc_num_layer=2, dec_num_layer=2, dec_num_unit=100, enc_num_unit=25, enc_act='elu', dec_act='linear', num_train_dec=5, num_train_enc=1, dropout=0.0, snr_test_start=-1.5, snr_test_end=4.0, snr_points=12, batch_size=100, num_epoch=1, test_ratio=1, block_len=100, block_len_low=10, block_len_high=200, is_variable_block_len=False, num_block=1000, test_channel_mode='block_norm', train_channel_mode='block_norm', enc_truncate_limit=0, no_code_norm=False, enc_quantize_level=2, enc_value_limit=1.0, enc_grad_limit=0.01, enc_clipping='both', optimizer='adam', dec_lr=0.001, enc_lr=0.001, no_cuda=False, rec_quantize=False, print_pos_ber=False, print_pos_power=False, print_test_traj=False, precompute_norm_stats=False, D=1)
use_cuda:  False
Channel_AE(
  (enc): ENC(
    (enc_rnn): GRU(1, 25, num_layers=2, batch_first=True)
    (enc_linear): Linear(in_features=25, out_features=3, bias=True)
  )
  (dec): DEC(
    (dropout): Dropout(p=0.0, inplace=False)
    (attention): Attention(
      (attn): Linear(in_features=200, out_features=1, bias=False)
    )
    (fc): Linear(in_features=200, out_features=100, bias=True)
    (dec1_rnns): GRU(3, 100, num_layers=2, batch_first=True)
    (dec2_rnns): GRU(3, 100, num_layers=2, batch_first=True)
    (dec_outputs): Linear(in_features=200, out_features=1, bias=True)
  )
)
====> Epoch: 1 Average loss: 0.69322006  running time 110.7803361415863
====> Epoch: 1 Average loss: 0.68214332  running time 111.10224103927612
====> Epoch: 1 Average loss: 0.64601197  running time 109.97724676132202
====> Epoch: 1 Average loss: 0.60917930  running time 110.42463946342468
====> Epoch: 1 Average loss: 0.60324341  running time 110.48130369186401
====> Epoch: 1 Average loss: 0.59937087  running time 109.97014999389648
====> Test set BCE loss 0.5862980484962463 Custom Loss 0.5862980484962463 with ber  0.3125600218772888 with bler  1.0
saved model C:\WorkSpace\Swetha_M20AIE317_MTP\model_test\attention_model_1_awgn_lr_0.001_D1_1000_20221217-185119.pt
each epoch training time: 667.9217972755432s
saved model C:\WorkSpace\Swetha_M20AIE317_MTP\model_test\attention_model_awgn_lr_0.001_D1_1000.pt
SNRS [-1.5, -1.0, -0.5, 0.0, 0.5, 1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0]
Test SNR -1.5 with ber  0.3502599895000458 with bler 1.0
Test SNR -1.0 with ber  0.34268003702163696 with bler 1.0
Test SNR -0.5 with ber  0.3325900435447693 with bler 1.0
Test SNR 0.0 with ber  0.3236500322818756 with bler 1.0
Test SNR 0.5 with ber  0.31759002804756165 with bler 1.0
Test SNR 1.0 with ber  0.31043997406959534 with bler 1.0
Test SNR 1.5 with ber  0.3016200065612793 with bler 1.0
Test SNR 2.0 with ber  0.2921999990940094 with bler 1.0
Test SNR 2.5 with ber  0.28533998131752014 with bler 1.0
Test SNR 3.0 with ber  0.2779499888420105 with bler 1.0
Test SNR 3.5 with ber  0.27149999141693115 with bler 1.0
Test SNR 4.0 with ber  0.26403000950813293 with bler 1.0
final results on SNRs  [-1.5, -1.0, -0.5, 0.0, 0.5, 1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0]
BER [0.3502599895000458, 0.34268003702163696, 0.3325900435447693, 0.3236500322818756, 0.31759002804756165, 0.31043997406959534, 0.3016200065612793, 0.2921999990940094, 0.28533998131752014, 0.2779499888420105, 0.27149999141693115, 0.26403000950813293]
BLER [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]
encoder power is 1.0
adjusted SNR should be [-1.4999997446509226, -1.0000000166986343, -0.49999973308696327, -0.0, 0.5000001308463472, 1.0000002900227403, 1.5000000201403676, 2.0000002404171053, 2.5000000877622415, 3.0000002493010487, 3.500000207085638, 3.999999717024358]
