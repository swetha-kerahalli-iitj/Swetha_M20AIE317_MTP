Namespace(channel='awgn', vv=5, radar_prob=0.05, radar_power=5.0, train_enc_channel_low=1.0, train_enc_channel_high=1.0, train_dec_channel_low=-1.5, train_dec_channel_high=2.0, init_nw_weight='default', code_rate_k=1, code_rate_n=3, enc_rnn='gru', dec_rnn='gru', enc_num_layer=2, dec_num_layer=2, dec_num_unit=100, enc_num_unit=25, enc_act='elu', dec_act='linear', num_train_dec=5, num_train_enc=1, dropout=0.0, snr_test_start=-1.5, snr_test_end=4.0, snr_points=12, batch_size=100, num_epoch=1, test_ratio=1, block_len=100, block_len_low=10, block_len_high=200, is_variable_block_len=False, num_block=1000, test_channel_mode='block_norm', train_channel_mode='block_norm', enc_truncate_limit=0, no_code_norm=False, enc_quantize_level=2, enc_value_limit=1.0, enc_grad_limit=0.01, enc_clipping='both', optimizer='adam', dec_lr=0.001, enc_lr=0.001, no_cuda=False, rec_quantize=False, print_pos_ber=False, print_pos_power=False, print_test_traj=False, precompute_norm_stats=False, D=1)
use_cuda:  False
Channel_AE(
  (enc): ENC(
    (enc_rnn): GRU(1, 25, num_layers=2, batch_first=True)
    (enc_linear): Linear(in_features=25, out_features=3, bias=True)
  )
  (dec): DEC(
    (dropout): Dropout(p=0.0, inplace=False)
    (attention): Attention(
      (attn): Linear(in_features=200, out_features=1, bias=False)
    )
    (fc): Linear(in_features=200, out_features=100, bias=True)
    (dec1_rnns): GRU(3, 100, num_layers=2, batch_first=True)
    (dec2_rnns): GRU(3, 100, num_layers=2, batch_first=True)
    (dec_outputs): Linear(in_features=200, out_features=1, bias=True)
  )
)
====> Epoch: 1 Average loss: 0.69334677  running time 90.22919368743896
====> Epoch: 1 Average loss: 0.69197841  running time 89.03415703773499
====> Epoch: 1 Average loss: 0.68783600  running time 88.40298533439636
====> Epoch: 1 Average loss: 0.68388792  running time 88.69675350189209
====> Epoch: 1 Average loss: 0.68349950  running time 88.2532274723053
====> Epoch: 1 Average loss: 0.68336434  running time 88.60805487632751
====> Test set BCE loss 0.680479109287262 Custom Loss 0.680479109287262 with ber  0.43481001257896423 with bler  1.0
saved model C:\WorkSpace\Swetha_M20AIE317_MTP\model_test\attention_model_1_awgn_lr_0.001_D1_1000_20221217-075924.pt
each epoch training time: 537.40465259552s
saved model C:\WorkSpace\Swetha_M20AIE317_MTP\model_test\attention_model_awgn_lr_0.001_D1_1000.pt
SNRS [-1.5, -1.0, -0.5, 0.0, 0.5, 1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0]
Test SNR -1.5 with ber  0.4542199969291687 with bler 1.0
Test SNR -1.0 with ber  0.4489099383354187 with bler 1.0
Test SNR -0.5 with ber  0.4427899718284607 with bler 1.0
Test SNR 0.0 with ber  0.44200998544692993 with bler 1.0
Test SNR 0.5 with ber  0.4379200041294098 with bler 1.0
Test SNR 1.0 with ber  0.4349500238895416 with bler 1.0
Test SNR 1.5 with ber  0.434609979391098 with bler 1.0
Test SNR 2.0 with ber  0.4306800365447998 with bler 1.0
Test SNR 2.5 with ber  0.4258200228214264 with bler 1.0
Test SNR 3.0 with ber  0.41950997710227966 with bler 1.0
Test SNR 3.5 with ber  0.4166699945926666 with bler 1.0
Test SNR 4.0 with ber  0.4147999882698059 with bler 1.0
final results on SNRs  [-1.5, -1.0, -0.5, 0.0, 0.5, 1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0]
BER [0.4542199969291687, 0.4489099383354187, 0.4427899718284607, 0.44200998544692993, 0.4379200041294098, 0.4349500238895416, 0.434609979391098, 0.4306800365447998, 0.4258200228214264, 0.41950997710227966, 0.4166699945926666, 0.4147999882698059]
BLER [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]
encoder power is 1.0
adjusted SNR should be [-1.4999997446509226, -1.0000000166986343, -0.49999973308696327, -0.0, 0.5000001308463472, 1.0000002900227403, 1.5000000201403676, 2.0000002404171053, 2.5000000877622415, 3.0000002493010487, 3.500000207085638, 3.999999717024358]
